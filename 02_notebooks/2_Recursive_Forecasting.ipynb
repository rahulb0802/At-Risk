{"cells":[{"cell_type":"markdown","source":["## Configuration"],"metadata":{"id":"W-pSs-LMs1Qt"}},{"cell_type":"markdown","source":["This cell contains all key parameters for analysis. To replicate the paper's results, run this cell and \"Run All\"."],"metadata":{"id":"c7y2R6sgs4Qd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sRmu2MCx4Li"},"outputs":[],"source":["# Standard Imports\n","import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import joblib\n","\n","# Machine Learning and Preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.impute import KNNImputer\n","from sklearn.linear_model import LogisticRegression\n","import xgboost as xgb\n","from sklearn.ensemble import HistGradientBoostingClassifier as HGBoost\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.decomposition import PCA\n","from scipy.stats import pointbiserialr\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n","from sklearn.metrics import brier_score_loss, make_scorer, average_precision_score, log_loss, matthews_corrcoef\n","from sklearn.base import clone\n","import statsmodels.api as sm\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","\n","\n","# Google Drive Mounting and Paths\n","from google.colab import drive\n","drive.mount('/content/drive')\n","BASE_PATH = \"/content/drive/MyDrive/Diffusion_Indices_Project/\"\n","INTERMEDIATE_PATH = os.path.join(BASE_PATH, \"03_intermediate_data\")\n","RESULTS_PATH = os.path.join(BASE_PATH, \"04_results\")\n","OOS_PRED_PATH = os.path.join(RESULTS_PATH, \"oos_predictions\")\n","SUB_INDICES_PATH = os.path.join(RESULTS_PATH, 'sub_indices_for_tuning')\n","os.makedirs(SUB_INDICES_PATH, exist_ok=True)\n","\n","# Create the results directories if they don't exist\n","os.makedirs(RESULTS_PATH, exist_ok=True)\n","os.makedirs(OOS_PRED_PATH, exist_ok=True)\n","\n","# Out-of-sample (OOS) Loop Settings\n","OOS_START_DATE = '1990-01-01'\n","PREDICTION_HORIZONS = [1, 3]\n","LAGS_TO_ADD = [1, 2, 3, 6, 9, 12]\n","\n","\n","# These are the five models used to generate the ensemble forecasts\n","MODELS_TO_RUN = {\n","    'Logit': LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000, random_state=42),\n","    'Logit_L1': LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42),\n","    'HGBoost': HGBoost(random_state=42),\n","    'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n","    'RandomForest': RandomForestClassifier(random_state=42),\n","}\n","\n","# Rerun Control\n","# Set to True to delete and regenerate all results files.\n","FORCE_RERUN_ALL_SETS = False\n","FORCE_RERUN_SPECIFIC_SETS = ['Deter', 'PCA_Factors_8'] # This is ignored if the RERUN_ALL_SETS is True"]},{"cell_type":"markdown","source":["## Loading Data"],"metadata":{"id":"DP0GRbtAtzws"}},{"cell_type":"markdown","source":["The following steps load data from the original notebook that preprocesses data."],"metadata":{"id":"sDLse4GRt2Oy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXQtcmHmGnHi"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bKKdfXwyI4t"},"outputs":[],"source":["print(\"Loading analysis-ready datasets...\")\n","y_target_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'y_target.pkl'))\n","X_yield_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_yield.pkl'))\n","X_transformed_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_transformed_monthly.pkl'))\n","X_untransformed_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_untransformed_monthly.pkl'))\n","X_ads_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_ads.pkl'))\n","tcodes = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'tcodes.pkl'))\n","\n","print(\"All data loaded successfully.\")\n","print(f\"Data shape: {X_transformed_full.shape}, Target shape: {y_target_full.shape}\")"]},{"cell_type":"code","source":["VARS_TO_REMOVE = ['ACOGNO', 'TWEXAFEGSMTHx', 'UMCSENTx', 'OILPRICEx']\n","\n","vars_that_exist_to_remove = [var for var in VARS_TO_REMOVE if var in X_transformed_full.columns]\n","\n","X_transformed_full = X_transformed_full.drop(columns=vars_that_exist_to_remove)\n","X_untransformed_full = X_untransformed_full.drop(columns=vars_that_exist_to_remove)\n","\n","print(f\"--- Data Filtering Complete ---\")\n","print(f\"Removed {len(vars_that_exist_to_remove)} problematic variables from all master DataFrames.\")\n","print(f\"The list of removed variables is: {vars_that_exist_to_remove}\")\n","print(f\"New data shape: {X_transformed_full.shape}\")"],"metadata":{"id":"1AB1K8yDb2oH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analyzing erratic behavior of tcode1 variables\n","tcode1_vars = ['TB6SMFFM', 'VIXCLSx', 'T1YFFM', 'TB3SMFFM', 'COMPAPFFx',\n","               'CES0600000007', 'T5YFFM', 'T10YFFM', 'AWHMAN', 'AAAFFM', 'BAAFFM']\n","\n","data = X_untransformed_full\n","y_target = y_target_full\n","\n","recession_starts = y_target[y_target['USRECM'].diff() == 1].index\n","\n","window_before = 12\n","window_after = 6\n","all_events = []\n","\n","for start_date in recession_starts:\n","    try:\n","        start_loc = data.index.get_loc(start_date)\n","\n","        if start_loc >= window_before:\n","            event_window = data.iloc[start_loc - window_before : start_loc + window_after + 1]\n","\n","            if event_window.empty:\n","                continue\n","\n","            normalized_window = event_window - event_window.iloc[0]\n","\n","            normalized_window.index = np.arange(-window_before, window_after + 1)\n","            all_events.append(normalized_window)\n","\n","    except KeyError:\n","\n","        print(f\"Recession start date {start_date} not found in data index. Skipping.\")\n","        continue\n","\n","if not all_events:\n","    print(\"Could not generate event study plot: No recessions with a full historical window were found in the sample.\")\n","else:\n","    # Average the trajectories\n","    average_event_trajectory = pd.concat(all_events).groupby(level=0).mean()\n","\n","    fig, ax = plt.subplots(figsize=(14, 9))\n","\n","    special_vars = ['VIXCLSx', 'AAAFFM', 'BAAFFM']\n","    other_vars = [v for v in tcode1_vars if v not in special_vars]\n","\n","    average_event_trajectory[special_vars].plot(ax=ax, linewidth=3)\n","    average_event_trajectory[other_vars].plot(ax=ax, style='--', color='grey', linewidth=1.5, legend=False)\n","\n","    ax.axvline(x=0, color='red', linestyle='--', label='Recession Start (T=0)')\n","    ax.set_title('Average Behavior of Stationary Variables Around Recessions', fontsize=20)\n","    ax.set_xlabel('Months Relative to Recession Start', fontsize=12)\n","    ax.set_ylabel('Change from T-12 Months (Levels)', fontsize=12)\n","    ax.grid(True, linestyle='--', alpha=0.6)\n","    ax.legend()\n","\n","    plt.show()"],"metadata":{"id":"e5e2P0BZSfMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# modeling interaction between real and fin to justify diffs for credit spreads\n","spread_vars = ['AAAFFM', 'BAAFFM', 'TB6SMFFM', 'T1YFFM', 'TB3SMFFM', 'COMPAPFFx', 'T5YFFM', 'T10YFFM']\n","\n","\n","in_sample_spreads = X_untransformed_full.loc[:'1989-12-31', spread_vars]\n","\n","\n","real_economy_indicator = np.log(X_untransformed_full['INDPRO']).diff().loc[:'1989-12-31']\n","\n","results = []\n","for var in spread_vars:\n","    level = in_sample_spreads[var]\n","    change = in_sample_spreads[var].diff()\n","\n","\n","    corr_level = pd.concat([real_economy_indicator, level], axis=1).dropna().corr().iloc[0, 1]\n","    corr_change = pd.concat([real_economy_indicator, change], axis=1).dropna().corr().iloc[0, 1]\n","\n","    # A high ratio means the \"change\" is more informative about the real economy\n","    if abs(corr_level) > 0:\n","        ratio = abs(corr_change) / abs(corr_level)\n","    else:\n","        ratio = np.nan\n","\n","    results.append({'Variable': var, 'Corr_Level': corr_level, 'Corr_Change': corr_change, 'Ratio': ratio})\n","\n","\n","results_df = pd.DataFrame(results).set_index('Variable')\n","print(\"Contagion Correlation Ratio (Change vs. Level with INDPRO Growth)\")\n","print(results_df.sort_values(by='Ratio', ascending=False))"],"metadata":{"id":"_eoQ6IzqsaLX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predictor Set Generation"],"metadata":{"id":"OC9ZkbBut8si"}},{"cell_type":"markdown","source":["Includes functions for selecting top variables per category, then creating index (two factor, just weakness, just deterioration)."],"metadata":{"id":"XreyYELV2Qt3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSNA-96GyhWN"},"outputs":[],"source":["lasso_weights_cache = {}\n","\n","def select_top_variables_per_category(X_data, y_data, variable_groups, horizon, top_n=10, corr_threshold=0.1):\n","    \"\"\"\n","    Selects the top N variables per category based on correlation.\n","    \"\"\"\n","    print(\"      -> Selecting top variables per category (Strict Top-N)...\")\n","\n","    y_shifted = y_data.shift(-horizon)\n","    y_shifted.name = 'y_lead'\n","\n","    aligned_data = pd.concat([y_shifted, X_data], axis=1).dropna()\n","    y_aligned = aligned_data['y_lead']\n","    X_aligned = aligned_data.drop(columns=['y_lead'])\n","\n","    if len(y_aligned.unique()) < 2:\n","        return {cat: vars[:top_n] for cat, vars in variable_groups.items()} # Fallback\n","\n","    refined_groups = {}\n","    for category, var_list in variable_groups.items():\n","        var_scores = {}\n","        for var in var_list:\n","            if var in X_aligned.columns:\n","                try:\n","                    correlation, _ = pointbiserialr(X_aligned[var], y_aligned)\n","                    if not np.isnan(correlation) and abs(correlation) >= corr_threshold:\n","                        var_scores[var] = abs(correlation)\n","                except ValueError:\n","                    continue\n","\n","        # Sort variables by score\n","        sorted_vars = sorted(var_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","\n","        # Extract only the names of the top N variables\n","        top_var_names = [var for var, score in sorted_vars[:top_n]]\n","\n","\n","        if top_var_names:\n","            refined_groups[category] = top_var_names # Store the list of names\n","\n","    return refined_groups\n","\n","\n","def get_final_definitive_span(X_transformed_data, variable_groups):\n","    \"\"\"\n","    Calculates the single, definitive, system-wide optimal span based on the\n","    median persistence of all variables actively used in the index\n","    \"\"\"\n","    print(\"Calculating the Final, Definitive System-Wide Optimal Span...\")\n","\n","    relevant_vars = [var for var_list in variable_groups.values() for var in var_list]\n","\n","    X_relevant = X_transformed_data[[v for v in relevant_vars if v in X_transformed_data.columns]]\n","\n","    # Calculate autocorrelation\n","    autocorrelations = X_relevant.apply(lambda s: s.autocorr(lag=1)).fillna(0).abs()\n","\n","    median_persistence = autocorrelations.median()\n","\n","    optimal_span = 1 / (median_persistence + 1e-9)\n","\n","    print(f\"\\n--- Definitive Results ---\")\n","    print(f\"Median Persistence of Relevant TRANSFORMED Signals: {median_persistence:.4f}\")\n","    print(f\"Calculated System-Wide Optimal Span: {optimal_span:.4f}\")\n","\n","    return optimal_span\n","\n","def generate_PCA_Factors(X_transformed_train, n_factors=8):\n","    \"\"\"\n","    Returns the top PCA factors.\n","    \"\"\"\n","    print(\"      -> Generating PCA Factors...\")\n","\n","    # Drop columns that are entirely NaN in the CURRENT training slice\n","    X_stat = X_transformed_train.copy()\n","    cols_to_drop_nan = X_stat.columns[X_stat.isna().all()]\n","    if not cols_to_drop_nan.empty:\n","        print(f\"\\n         ... Dropping {len(cols_to_drop_nan)} all-NaN columns: {cols_to_drop_nan.to_list()}\", end=\"\")\n","    X_stat_valid = X_stat.drop(columns=cols_to_drop_nan)\n","\n","\n","    # Imputation. Now guaranteed to have matching shapes\n","    imputer = KNNImputer(n_neighbors=5)\n","    X_imputed = pd.DataFrame(imputer.fit_transform(X_stat_valid),\n","                             index=X_stat_valid.index,\n","                             columns=X_stat_valid.columns)\n","\n","    # Standardization\n","    scaler = StandardScaler()\n","    X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed),\n","                            index=X_imputed.index,\n","                            columns=X_imputed.columns)\n","\n","    # Drop constant columns AFTER scaling (a final check)\n","    variances = X_scaled.var()\n","    constant_cols = variances[variances < 1e-10].index\n","    if not constant_cols.empty:\n","        print(f\"\\n         ... Dropping {len(constant_cols)} constant columns: {constant_cols.to_list()}\", end=\"\")\n","    X_final_for_pca = X_scaled.drop(columns=constant_cols)\n","\n","\n","    # PCA on the final, clean data\n","    pca = PCA(n_components=n_factors)\n","    factors = pca.fit_transform(X_final_for_pca)\n","\n","    pca_factors_df = pd.DataFrame(factors,\n","                                  index=X_final_for_pca.index,\n","                                  columns=[f'PCA_Factor_{i+1}' for i in range(n_factors)])\n","\n","    return pca_factors_df\n","\n","def generate_Targeted_PCA_Factors(X_transformed_train, selected_vars, n_factors=8):\n","    \"\"\"\n","    Returns the top PCA factors.\n","    \"\"\"\n","    print(\"      -> Generating Targeted PCA Factors...\")\n","    X_subset = X_transformed_train[selected_vars]\n","\n","    # Drop columns that are entirely NaN in the CURRENT training slice\n","    X_stat = X_subset.copy()\n","    cols_to_drop_nan = X_stat.columns[X_stat.isna().all()]\n","    if not cols_to_drop_nan.empty:\n","        print(f\"\\n         ... Dropping {len(cols_to_drop_nan)} all-NaN columns: {cols_to_drop_nan.to_list()}\", end=\"\")\n","    X_stat_valid = X_stat.drop(columns=cols_to_drop_nan)\n","\n","    # Imputation. Now guaranteed to have matching shapes\n","    imputer = KNNImputer(n_neighbors=5)\n","    X_imputed = pd.DataFrame(imputer.fit_transform(X_stat_valid),\n","                             index=X_stat_valid.index,\n","                             columns=X_stat_valid.columns)\n","\n","    # Standardization\n","    scaler = StandardScaler()\n","    X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed),\n","                            index=X_imputed.index,\n","                            columns=X_imputed.columns)\n","\n","    # Drop constant columns AFTER scaling (a final check)\n","    variances = X_scaled.var()\n","    constant_cols = variances[variances < 1e-10].index\n","    if not constant_cols.empty:\n","        print(f\"\\n         ... Dropping {len(constant_cols)} constant columns: {constant_cols.to_list()}\", end=\"\")\n","    X_final_for_pca = X_scaled.drop(columns=constant_cols)\n","\n","\n","    # PCA on the final, clean data\n","    pca = PCA(n_components=n_factors)\n","    factors = pca.fit_transform(X_final_for_pca)\n","\n","    pca_factors_df = pd.DataFrame(factors,\n","                                  index=X_final_for_pca.index,\n","                                  columns=[f'PCA_Factor_{i+1}' for i in range(n_factors)])\n","    return pca_factors_df\n","\n","def generate_TFDI_Sub_Indices(X_transformed_train, y_train, horizon):\n","    \"\"\"\n","    This is the TDFI framework. It uses Ridge (L2) for nowcasting (h<3)\n","    to retain all signals, and LASSO (L1) for forecasting (h>=3) to perform\n","    automated feature selection and remove noise.\n","    \"\"\"\n","    print(f\"      -> Generating TFDI (h={horizon})...\")\n","\n","    # Generate Unweighted Sub-Indices\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM'],\n","        'FX_Rates': ['EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'AAAFFM', 'BAAFFM', 'VIXCLSx'}\n","    special_financial_vars = {'AAAFFM', 'BAAFFM', 'VIXCLSx'}\n","    X_momentum = X_transformed_train.rolling(window=3, min_periods=1).mean()\n","    # refined_variable_groups = select_top_variables_per_category(X_momentum, y_train, variable_groups, horizon=horizon, top_n=10)\n","    refined_variable_groups = variable_groups\n","\n","    adaptive_window = 180\n","    weakness_states = pd.DataFrame(index=X_transformed_train.index)\n","    deterioration_states = pd.DataFrame(index=X_transformed_train.index)\n","\n","\n","    all_selected_vars = [var for var_list in refined_variable_groups.values() for var in var_list]\n","    for var in all_selected_vars:\n","        signal_for_ranking = X_transformed_train[var]\n","        is_counter_theoretical = var in counter_cyclical_vars\n","        use_counter_logic = is_counter_theoretical\n","\n","        if horizon == 1 and var in special_financial_vars:\n","            use_counter_logic = True\n","            signal_for_ranking = X_transformed_train[var].diff()\n","        elif horizon > 1 and var in special_financial_vars:\n","            signal_for_ranking = X_transformed_train[var].diff()\n","\n","        level_signal = signal_for_ranking\n","        momentum_signal = signal_for_ranking.rolling(window=3, min_periods=1).mean()\n","\n","\n","        weakness_threshold = level_signal.quantile(0.8 if use_counter_logic else 0.2)\n","        weak_state = pd.Series(0.0, index=level_signal.index)\n","        if use_counter_logic: weak_state[level_signal > weakness_threshold] = 1.0\n","        else: weak_state[level_signal < weakness_threshold] = 1.0\n","        weakness_states[var] = weak_state\n","\n","        deterioration_threshold = momentum_signal.quantile(0.8 if use_counter_logic else 0.2)\n","        deteriorating_state = pd.Series(0.0, index=momentum_signal.index)\n","        if use_counter_logic: deteriorating_state[momentum_signal > deterioration_threshold] = 1.0\n","        else: deteriorating_state[momentum_signal < deterioration_threshold] = 1.0\n","        deterioration_states[var] = deteriorating_state\n","\n","\n","    # Aggregate into Unweighted Per-Category Sub-Indices\n","    cat_weakness_di = pd.DataFrame(index=X_transformed_train.index)\n","    cat_deterioration_di = pd.DataFrame(index=X_transformed_train.index)\n","\n","    for category, var_list in refined_variable_groups.items():\n","        # Only try to access columns that were actually selected\n","        weak_cols = [v for v in var_list if v in weakness_states.columns]\n","        if weak_cols:\n","            cat_weakness_di[f\"W_{category.replace(' ', '_')}\"] = weakness_states[weak_cols].mean(axis=1)\n","\n","        det_cols = [v for v in var_list if v in deterioration_states.columns]\n","        if det_cols:\n","            cat_deterioration_di[f\"D_{category.replace(' ', '_')}\"] = deterioration_states[det_cols].mean(axis=1)\n","\n","\n","    all_sub_indices = pd.concat([cat_weakness_di, cat_deterioration_di], axis=1)\n","    # Weighting step\n","    y_shifted = y_train.shift(-horizon).rename('y_lead')\n","    weighting_data = pd.concat([y_shifted, all_sub_indices], axis=1, join='inner').dropna()\n","    y_weight = weighting_data['y_lead']\n","    X_weight = weighting_data.drop(columns=['y_lead'])\n","\n","    weights = pd.Series(1.0, index=X_weight.columns) # Default to equal weights\n","\n","    if len(y_weight.unique()) == 2 and not X_weight.empty:\n","\n","\n","        if horizon < 3:\n","            print(\"         ... using LASSO (L1) weighting for forecasting.\")\n","            model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, C=0.1)\n","            param_grid = {'alpha': [0.1, 1.0, 10.0, 50.0, 100.0]}\n","        else:\n","            print(\"         ... using LASSO (L1) weighting for forecasting.\")\n","            model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, C=0.1)\n","            param_grid = {'C': [1.0, 0.5, 0.1, 0.05, 0.01]}\n","\n","        # Not fitting gridsearch because tuning does worse\n","        model.fit(X_weight, y_weight)\n","        weights = pd.Series(np.abs(model.coef_[0]), index=X_weight.columns)\n","\n","    return all_sub_indices.mul(weights).fillna(method='ffill').fillna(0)\n","\n","def generate_Weakness_Indices(X_transformed_train, y_train, horizon):\n","    \"\"\"\n","    This is the final, unified framework. It uses Ridge (L2) for nowcasting (h<3)\n","    to retain all signals, and LASSO (L1) for forecasting (h>=3) to perform\n","    automated feature selection and remove noise.\n","    \"\"\"\n","    print(f\"      -> Generating TFDI Weakness Indices (h={horizon})...\")\n","\n","\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM'],\n","        'FX_Rates': ['EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'AAAFFM', 'BAAFFM', 'VIXCLSx'}\n","    special_financial_vars = {'AAAFFM', 'BAAFFM', 'VIXCLSx'}\n","    X_momentum = X_transformed_train.rolling(window=3, min_periods=1).mean()\n","    # refined_variable_groups = select_top_variables_per_category(X_momentum, y_train, variable_groups, horizon=horizon, top_n=10)\n","    refined_variable_groups = variable_groups\n","    adaptive_window = 180\n","\n","    weakness_states = pd.DataFrame(index=X_transformed_train.index)\n","    deterioration_states = pd.DataFrame(index=X_transformed_train.index)\n","\n","    all_selected_vars = [var for var_list in refined_variable_groups.values() for var in var_list]\n","    for var in all_selected_vars:\n","        signal_for_ranking = X_transformed_train[var]\n","        is_counter_theoretical = var in counter_cyclical_vars\n","        use_counter_logic = is_counter_theoretical\n","\n","        if horizon == 1 and var in special_financial_vars:\n","            use_counter_logic = True\n","            signal_for_ranking = X_transformed_train[var].diff()\n","        elif horizon > 1 and var in special_financial_vars:\n","            signal_for_ranking = X_transformed_train[var].diff()\n","\n","        level_signal = signal_for_ranking\n","        momentum_signal = signal_for_ranking.rolling(window=3, min_periods=1).mean()\n","\n","\n","        weakness_threshold = level_signal.rolling(window=adaptive_window, min_periods=36).quantile(0.8 if use_counter_logic else 0.2)\n","        weak_state = pd.Series(0.0, index=level_signal.index)\n","        if use_counter_logic: weak_state[level_signal > weakness_threshold] = 1.0\n","        else: weak_state[level_signal < weakness_threshold] = 1.0\n","        weakness_states[var] = weak_state\n","\n","\n","    # Perform same operations as original function but only for Weakness\n","    cat_weakness_di = pd.DataFrame(index=X_transformed_train.index)\n","\n","    for category, var_list in refined_variable_groups.items():\n","\n","        weak_cols = [v for v in var_list if v in weakness_states.columns]\n","        if weak_cols:\n","            cat_weakness_di[f\"W_{category.replace(' ', '_')}\"] = weakness_states[weak_cols].mean(axis=1)\n","\n","\n","\n","    all_sub_indices = cat_weakness_di\n","\n","    y_shifted = y_train.shift(-horizon).rename('y_lead')\n","    weighting_data = pd.concat([y_shifted, all_sub_indices], axis=1, join='inner').dropna()\n","    y_weight = weighting_data['y_lead']\n","    X_weight = weighting_data.drop(columns=['y_lead'])\n","\n","    weights = pd.Series(1.0, index=X_weight.columns) # Default to equal weights\n","\n","    if len(y_weight.unique()) == 2 and not X_weight.empty:\n","\n","\n","\n","        if horizon < 3:\n","            print(\"         ... using LASSO (L2) weighting for forecasting.\")\n","            model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, C=0.1)\n","            param_grid = {'alpha': [0.1, 1.0, 10.0, 50.0, 100.0]}\n","        else:\n","            print(\"         ... using LASSO (L1) weighting for forecasting.\")\n","            model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, C=0.1)\n","            param_grid = {'C': [1.0, 0.5, 0.1, 0.05, 0.01]}\n","\n","        # Not fitting gridsearch because tuning does worse\n","        model.fit(X_weight, y_weight)\n","        weights = pd.Series(np.abs(model.coef_[0]), index=X_weight.columns)\n","\n","\n","    return all_sub_indices.mul(weights).fillna(method='ffill').fillna(0)\n","\n","\n","def generate_Deter_Indices(X_transformed_train, y_train, horizon, var_spans):\n","    \"\"\"\n","    This is the final, unified framework. It uses Ridge (L2) for nowcasting (h<3)\n","    to retain all signals, and LASSO (L1) for forecasting (h>=3) to perform\n","    automated feature selection and remove noise.\n","    \"\"\"\n","    print(f\"      -> Generating Deterioration Indices (h={horizon})...\")\n","\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM', 'EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'VIXCLSx', 'BAAFFM', 'AAAFFM'}\n","    special_financial_vars = {'VIXCLSx', 'BAAFFM', 'AAAFFM'}\n","\n","    deterioration_states = pd.DataFrame(index=X_transformed_train.index)\n","    momentum_signals = pd.DataFrame(index=X_transformed_train.index)\n","\n","    all_selected_vars = [var for var_list in variable_groups.values() for var in var_list]\n","    for var in all_selected_vars:\n","        signal_for_ranking = X_transformed_train[var]\n","        is_counter_theoretical = var in counter_cyclical_vars\n","        use_counter_logic = is_counter_theoretical\n","\n","        # Use transformed values for special financial vars, not levels\n","        if var in special_financial_vars:\n","            signal_for_ranking = X_transformed_train[var].diff()\n","\n","        span = var_spans\n","        momentum_signal = signal_for_ranking.ewm(span=var_spans, adjust=False).mean()\n","\n","        deterioration_threshold = momentum_signal.quantile(0.75 if use_counter_logic else 0.25)\n","        deteriorating_state = pd.Series(0.0, index=momentum_signal.index)\n","        if use_counter_logic: deteriorating_state[momentum_signal > deterioration_threshold] = 1.0\n","        else: deteriorating_state[momentum_signal < deterioration_threshold] = 1.0\n","        deterioration_states[var] = deteriorating_state\n","\n","\n","\n","    cat_deterioration_di = pd.DataFrame(index=X_transformed_train.index)\n","    y_shifted = y_train.shift(-horizon)\n","\n","\n","    all_weights = {}\n","\n","    for category, var_list in variable_groups.items():\n","\n","        det_cols = [v for v in var_list if v in deterioration_states.columns]\n","        signals_in_cat = deterioration_states[det_cols]\n","\n","\n","        # Weights are determined by corr * autocorr\n","        data_for_corr = pd.concat([y_shifted, signals_in_cat], axis=1).dropna()\n","\n","        if not data_for_corr.empty and data_for_corr.iloc[:, 0].nunique() > 1:\n","            predictive_power = data_for_corr.corr().iloc[0, 1:].abs()\n","        else:\n","            predictive_power = pd.Series(1.0, index=signals_in_cat.columns)\n","\n","        stability = signals_in_cat.apply(lambda s: s.autocorr(lag=1)).fillna(0)\n","\n","        all_weights[category] = predictive_power * stability\n","\n","\n","    # Overall weight is average variable weight within cat\n","    category_importance_scores = {cat: weights.mean() for cat, weights in all_weights.items()}\n","\n","    all_weights_for_analysis = []\n","\n","    for category, var_list in variable_groups.items():\n","        det_cols = [v for v in var_list if v in deterioration_states.columns]\n","\n","        signals_in_cat = deterioration_states[det_cols]\n","        weights_in_cat = all_weights.get(category)\n","\n","        all_weights_for_analysis.append(weights_in_cat)\n","\n","        weighted_signals = signals_in_cat * weights_in_cat\n","        proportional_breadth_index = weighted_signals.mean(axis=1)\n","\n","        category_amplifier = category_importance_scores.get(category, 1.0)\n","\n","        if horizon == 1:\n","            final_index = proportional_breadth_index\n","        else:\n","            # Multiply weights\n","            final_index = proportional_breadth_index * category_amplifier\n","\n","\n","        cat_deterioration_di[f\"D_{category.replace(' ', '_')}\"] = final_index\n","\n","\n","\n","    all_sub_indices = cat_deterioration_di.ffill()\n","\n","    final_weights_df = pd.concat(all_weights_for_analysis)\n","\n","\n","\n","    return all_sub_indices, final_weights_df\n","\n","\n","\n","\n","def add_lags(df, lags_to_add, prefix=''):\n","    \"\"\"\n","    Adds lagged versions of columns to a DataFrame.\n","    \"\"\"\n","    if not lags_to_add:\n","        return df\n","\n","    df_lagged = df.copy()\n","    for lag in lags_to_add:\n","        df_shifted = df.shift(lag)\n","        df_shifted.columns = [f'{prefix}{col}_lag{lag}' for col in df.columns]\n","        df_lagged = pd.concat([df_lagged, df_shifted], axis=1)\n","\n","    return df_lagged"]},{"cell_type":"code","source":["variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM', 'EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }"],"metadata":{"id":"idD1QJfmUaug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Main Forecasting Loop"],"metadata":{"id":"8qU2-P8evGvX"}},{"cell_type":"markdown","source":["Parameters for the loop (such as horizons, lags, models, rerun control, etc.) can be found in the config cell."],"metadata":{"id":"2Re5NJCz2ct5"}},{"cell_type":"markdown","source":["To add a new predictor set:\n","\n","---\n","1. Create generator function in previous section.\n","2. Add to `ALL_POSSIBLE_SETS` array.\n","3. Add an `if` block in the predictor set generation section of the loop. Follow format of other sets.\n","4. Run the loop. It should detect that there is a new predictor set and only run that one, adding it to existing results (unless `FORCE_RERUN_ALL_SETS = True`).\n"],"metadata":{"id":"jtdkK4mx3PD1"}},{"cell_type":"markdown","source":["\n","\n","*   To rerun all sets, set `FORCE_RERUN_ALL_SETS` to `True`. This will ignore saved files and generate new results from scratch.\n","*   To rerun specific predictor set, add it to the `FORCE_RERUN_SPECIFIC_SETS` array.\n","\n","\n","\n"],"metadata":{"id":"cd_NKIiL2xMU"}},{"cell_type":"code","source":["print(\"\\nStep 4: Starting recursive out-of-sample forecasting loop...\")\n","\n","SAVE_MODELS = True\n","OOS_MODELS_PATH = os.path.join(RESULTS_PATH, 'models')\n","\n","oos_deter_indices_history = []\n","\n","IN_SAMPLE_END_DATE = '1989-12-31'\n","X_in_sample = X_transformed_full.loc[:IN_SAMPLE_END_DATE]\n","\n","var_spans_in_sample = get_final_definitive_span(X_in_sample, variable_groups)\n","\n","# Master Loop for All Horizons\n","for PREDICTION_HORIZON in PREDICTION_HORIZONS:\n","    print(f\"\\n{'='*25} Processing Horizon h={PREDICTION_HORIZON} {'='*25}\")\n","\n","    file_path = os.path.join(OOS_PRED_PATH, f'oos_results_h{PREDICTION_HORIZON}.pkl')\n","\n","    horizon_sub_index_path = os.path.join(SUB_INDICES_PATH, f'h{PREDICTION_HORIZON}')\n","    os.makedirs(horizon_sub_index_path, exist_ok=True)\n","\n","    if SAVE_MODELS:\n","        horizon_model_path = os.path.join(OOS_MODELS_PATH, f'h{PREDICTION_HORIZON}')\n","        os.makedirs(horizon_model_path, exist_ok=True)\n","        print(f\"Models for h = {PREDICTION_HORIZON} will be saved to: {horizon_model_path}\")\n","\n","    # Load existing results or initialize new ones\n","    oos_probs, oos_errors, oos_actuals = {}, {}, None # Start with empty dicts\n","    if not FORCE_RERUN_ALL_SETS:\n","        try:\n","            print(f\"Attempting to load existing results from: {file_path}\")\n","            oos_results = joblib.load(file_path)\n","            oos_probs = oos_results.get('probabilities', {}) # Use .get for safety\n","            oos_errors = oos_results.get('squared_errors', {})\n","            oos_actuals = oos_results.get('actuals', None)\n","            print(\"Successfully loaded existing results.\")\n","        except FileNotFoundError:\n","            print(\"No existing results file found. Initializing new structure.\")\n","    else:\n","        print(f\"FORCE_RERUN_ALL_SETS is True. Any loaded results will be ignored for this horizon.\")\n","\n","    # Determine which predictor sets need to be run\n","    ALL_POSSIBLE_SETS = ['Yield', 'Full', 'PCA_Factors_8', 'ADS', 'Deter']\n","\n","\n","    sets_to_run = []\n","    if FORCE_RERUN_ALL_SETS:\n","        sets_to_run = ALL_POSSIBLE_SETS\n","        print(f\"All {len(sets_to_run)} predictor sets will be re-run.\")\n","    else:\n","        for pred_set in ALL_POSSIBLE_SETS:\n","            # Rerun if not found or if specifically requested\n","            if pred_set not in oos_probs or pred_set in FORCE_RERUN_SPECIFIC_SETS:\n","                sets_to_run.append(pred_set)\n","        if FORCE_RERUN_SPECIFIC_SETS:\n","             print(f\"Will force rerun for specific sets: {FORCE_RERUN_SPECIFIC_SETS}\")\n","\n","    if not sets_to_run:\n","        print(\"All specified predictor sets have already been run for this horizon. Skipping.\")\n","        continue\n","\n","    print(f\"The following {len(sets_to_run)} sets will be run: {sets_to_run}\")\n","\n","    # Initialize/clear storage ONLY for the sets that are being run\n","    for pred_set in sets_to_run:\n","        oos_probs[pred_set] = {m: [] for m in MODELS_TO_RUN}\n","        oos_errors[pred_set] = {m: [] for m in MODELS_TO_RUN}\n","\n","    # Main time-series loop\n","    all_dates = y_target_full.index\n","    forecast_dates = all_dates[all_dates >= pd.to_datetime(OOS_START_DATE)]\n","\n","    if oos_actuals is None or (len(oos_actuals) != len(forecast_dates)):\n","        oos_actuals = y_target_full.loc[forecast_dates, 'USRECM']\n","        oos_actuals.index.name = 'Date'\n","\n","    start_time = time.time()\n","\n","    for i, forecast_date in enumerate(forecast_dates):\n","        iter_start_time = time.time()\n","        train_end_date = forecast_date - pd.DateOffset(months=PREDICTION_HORIZON)\n","        y_train_full = y_target_full.loc[:train_end_date, 'USRECM']\n","        y_actual = oos_actuals.loc[forecast_date]\n","\n","        print(f\"Iter {i+1}/{len(forecast_dates)}: h={PREDICTION_HORIZON}, Date={forecast_date.date()}... \", end=\"\")\n","\n","        # Centralized Data Preparation\n","        X_train_transformed_slice = X_transformed_full.loc[:train_end_date].copy()\n","        X_untransformed_slice = X_untransformed_full.loc[:train_end_date].copy()\n","        X_train_valid_cols = X_train_transformed_slice.drop(columns=X_train_transformed_slice.columns[X_train_transformed_slice.isna().all()]).copy()\n","        imputer_base = KNNImputer(n_neighbors=5)\n","        X_train_imputed = pd.DataFrame(imputer_base.fit_transform(X_train_valid_cols), index=X_train_valid_cols.index, columns=X_train_valid_cols.columns)\n","        scaler = StandardScaler()\n","        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), index=X_train_imputed.index, columns=X_train_imputed.columns)\n","\n","\n","        # Generate Predictor Sets\n","        predictor_data_iter = {}\n","\n","        if 'ADS' in sets_to_run:\n","            predictor_data_iter['ADS'] = X_ads_full.loc[:train_end_date]\n","        if 'Yield' in sets_to_run:\n","            predictor_data_iter['Yield'] = X_yield_full.loc[:train_end_date]\n","        if 'Full' in sets_to_run: predictor_data_iter['Full'] = X_train_scaled\n","        if 'PCA_Factors_8' in sets_to_run:\n","            # The function does all preprocessing internally\n","            all_8_factors = generate_PCA_Factors(X_train_transformed_slice, n_factors=8)\n","            predictor_data_iter['PCA_Factors_8'] = all_8_factors\n","        if 'TFDI' in sets_to_run:\n","            predictor_data_iter['TFDI'] = generate_TFDI_Sub_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON)\n","        if 'Weakness' in sets_to_run:\n","            predictor_data_iter['Weakness'] = generate_Weakness_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON)\n","        if 'Deter' in sets_to_run:\n","\n","            deter_index, avg_weights = generate_Deter_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON, var_spans=var_spans_in_sample)\n","\n","            latest_indices = deter_index.iloc[[-1]].copy()\n","            latest_indices.index = [forecast_date]\n","\n","            oos_deter_indices_history.append(latest_indices)\n","            predictor_data_iter['Deter'] = deter_index\n","\n","\n","        # Loop over the INTENDED sets\n","        for pred_set_name in sets_to_run:\n","            X_train_raw = predictor_data_iter.get(pred_set_name)\n","\n","            if X_train_raw is None or X_train_raw.empty:\n","                for model_name in MODELS_TO_RUN:\n","                    oos_probs[pred_set_name][model_name].append(np.nan)\n","                    oos_errors[pred_set_name][model_name].append(np.nan)\n","                continue\n","            if pred_set_name == 'OL_DW_Sub_Indices':\n","              X_train_lagged = X_train_raw\n","            else:\n","              X_train_lagged = add_lags(X_train_raw, LAGS_TO_ADD, prefix=f'{pred_set_name}_')\n","            X_predict_point = X_train_lagged.iloc[[-1]]\n","            X_train_final = X_train_lagged.dropna()\n","\n","            for model_name, model_template in MODELS_TO_RUN.items():\n","                prob, error = np.nan, np.nan\n","\n","                try:\n","                    common_index = y_train_full.index.intersection(X_train_final.index)\n","                    y_train_aligned = y_train_full.loc[common_index]\n","                    X_train_aligned = X_train_final.loc[common_index]\n","\n","                    if len(X_train_aligned) > max(LAGS_TO_ADD, default=0) + 20:\n","                        model_instance = clone(model_template)\n","\n","                        if 'XGBoost' in model_name:\n","                            neg, pos = (y_train_aligned == 0).sum(), (y_train_aligned == 1).sum()\n","                            if pos > 0: model_instance = model_template.set_params(scale_pos_weight=(neg/pos))\n","                        elif 'HGBoost' in model_name or 'RandomForest' in model_name or 'Logit' in model_name:\n","                            model_instance = model_template.set_params(class_weight='balanced')\n","\n","                        X_predict_imputed = X_predict_point.reindex(columns=X_train_aligned.columns).fillna(X_train_aligned.mean())\n","\n","                        if not X_predict_imputed.isna().any().any():\n","                            model_instance.fit(X_train_aligned, y_train_aligned)\n","\n","                            if 'SAVE_MODELS' and forecast_date == forecast_dates[-1]:\n","                                print(\"Last iteration. Saving model...\")\n","                                model_filename = f'{pred_set_name}_{model_name}.pkl'\n","                                model_path = os.path.join(horizon_model_path, model_filename)\n","                                joblib.dump(model_instance, model_path)\n","\n","                            prob = model_instance.predict_proba(X_predict_imputed)[:, 1][0]\n","                            error = (y_actual - prob)**2\n","                except Exception:\n","                    pass\n","\n","                oos_probs[pred_set_name][model_name].append(prob)\n","                oos_errors[pred_set_name][model_name].append(error)\n","\n","        iter_end_time = time.time()\n","        print(f\" ({(iter_end_time - iter_start_time):.2f}s)\")\n","\n","    # Save results after each horizon's loop is complete\n","    print(f\"\\n--- Loop for h={PREDICTION_HORIZON} Finished ---\")\n","    results_to_save = {'probabilities': oos_probs, 'squared_errors': oos_errors, 'actuals': oos_actuals}\n","    joblib.dump(results_to_save, file_path)\n","    print(f\"Updated results saved to: {file_path}\")\n","\n","print(\"\\n--- All Horizons Complete ---\")"],"metadata":{"id":"W1pVM-x92u-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# weights for each variable, averaged over sample\n","avg_weights = avg_weights.groupby(level=0).mean()\n","top_20_vars = avg_weights.sort_values(ascending=False).head(20)\n","\n","# Create the bar plot\n","plt.figure(figsize=(10, 12))\n","top_20_vars.sort_values().plot(kind='barh')\n","plt.title('Top 20 Most Important Variables (Average Weight)', fontsize=16)\n","plt.xlabel('Average Dynamic Stability Weight')\n","plt.grid(axis='x', linestyle='--')\n","plt.show()"],"metadata":{"id":"0FQ4bXMaeUXQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"18T8ENtyOeY6oZ-3Jk9e__GduSEjCUfk2","authorship_tag":"ABX9TyPv1tK778zmsnDLjQayhWRk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}