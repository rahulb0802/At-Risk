{"cells":[{"cell_type":"markdown","metadata":{"id":"W-pSs-LMs1Qt"},"source":["## Configuration"]},{"cell_type":"markdown","metadata":{"id":"c7y2R6sgs4Qd"},"source":["This cell contains all key parameters for analysis. To replicate the paper's results, run this cell and \"Run All\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sRmu2MCx4Li"},"outputs":[],"source":["# Standard Imports\n","import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import joblib\n","\n","# Machine Learning and Preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.impute import KNNImputer\n","from sklearn.linear_model import LogisticRegression\n","import xgboost as xgb\n","from sklearn.ensemble import HistGradientBoostingClassifier as HGBoost\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.decomposition import PCA\n","from scipy.stats import pointbiserialr\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n","from sklearn.metrics import brier_score_loss, make_scorer, average_precision_score, log_loss, matthews_corrcoef, f1_score\n","from sklearn.base import clone\n","import statsmodels.api as sm\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","import shap\n","\n","\n","# Google Drive Mounting and Paths\n","from google.colab import drive\n","drive.mount('/content/drive')\n","BASE_PATH = \"/content/drive/MyDrive/Diffusion_Indices_Project/\"\n","INTERMEDIATE_PATH = os.path.join(BASE_PATH, \"03_intermediate_data\")\n","RESULTS_PATH = os.path.join(BASE_PATH, \"04_results\")\n","OOS_PRED_PATH = os.path.join(RESULTS_PATH, \"oos_predictions\")\n","SUB_INDICES_PATH = os.path.join(RESULTS_PATH, 'sub_indices_for_tuning')\n","os.makedirs(SUB_INDICES_PATH, exist_ok=True)\n","\n","# Create the results directories if they don't exist\n","os.makedirs(RESULTS_PATH, exist_ok=True)\n","os.makedirs(OOS_PRED_PATH, exist_ok=True)\n","\n","# Out-of-sample (OOS) Loop Settings\n","OOS_START_DATE = '1990-01-01'\n","PREDICTION_HORIZONS = [6, 12]\n","LAGS_TO_ADD = [1, 3, 6]\n","\n","ngrid = 30\n","C_values = np.logspace(-3, 1, ngrid)\n","\n","# These are the five models used to generate the ensemble forecasts\n","MODELS_TO_RUN = {\n","    'Logit': LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000, random_state=42),\n","    'Logit_L1': LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42),\n","    'Logit_L2': LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42),\n","    # 'Logit_ElasticNet': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000),\n","    'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False),\n","}\n","\n","# Rerun Control\n","# Set to True to delete and regenerate all results files.\n","FORCE_RERUN_ALL_SETS = False\n","FORCE_RERUN_SPECIFIC_SETS = ['Deter_States', 'Deter_PCA', 'Deter', 'Deter_Avg', 'PCA_Factors_8', 'Full'] # This is ignored if the RERUN_ALL_SETS is True"]},{"cell_type":"markdown","metadata":{"id":"DP0GRbtAtzws"},"source":["## Loading Data"]},{"cell_type":"markdown","metadata":{"id":"sDLse4GRt2Oy"},"source":["The following steps load data from the original notebook that preprocesses data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXQtcmHmGnHi"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bKKdfXwyI4t"},"outputs":[],"source":["print(\"Loading analysis-ready datasets...\")\n","y_target_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'y_target.pkl'))\n","X_yield_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_yield.pkl'))\n","X_transformed_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_transformed_monthly.pkl'))\n","X_untransformed_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_untransformed_monthly.pkl'))\n","X_ads_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_ads.pkl'))\n","tcodes = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'tcodes.pkl'))\n","\n","print(\"All data loaded successfully.\")\n","print(f\"Data shape: {X_transformed_full.shape}, Target shape: {y_target_full.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AB1K8yDb2oH"},"outputs":[],"source":["VARS_TO_REMOVE = ['ACOGNO', 'TWEXAFEGSMTHx', 'UMCSENTx', 'OILPRICEx']\n","\n","vars_that_exist_to_remove = [var for var in VARS_TO_REMOVE if var in X_transformed_full.columns]\n","\n","X_transformed_full = X_transformed_full.drop(columns=vars_that_exist_to_remove)\n","X_untransformed_full = X_untransformed_full.drop(columns=vars_that_exist_to_remove)\n","\n","print(f\"--- Data Filtering Complete ---\")\n","print(f\"Removed {len(vars_that_exist_to_remove)} problematic variables from all master DataFrames.\")\n","print(f\"The list of removed variables is: {vars_that_exist_to_remove}\")\n","print(f\"New data shape: {X_transformed_full.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5e2P0BZSfMF"},"outputs":[],"source":["# analyzing erratic behavior of tcode1 variables\n","tcode1_vars = ['TB6SMFFM', 'VIXCLSx', 'T1YFFM', 'TB3SMFFM', 'COMPAPFFx',\n","               'CES0600000007', 'T5YFFM', 'T10YFFM', 'AWHMAN', 'AAAFFM', 'BAAFFM']\n","\n","data = X_untransformed_full\n","y_target = y_target_full\n","\n","recession_starts = y_target[y_target['USRECM'].diff() == 1].index\n","\n","window_before = 12\n","window_after = 6\n","all_events = []\n","\n","for start_date in recession_starts:\n","    try:\n","        start_loc = data.index.get_loc(start_date)\n","\n","        if start_loc >= window_before:\n","            event_window = data.iloc[start_loc - window_before : start_loc + window_after + 1]\n","\n","            if event_window.empty:\n","                continue\n","\n","            normalized_window = event_window - event_window.iloc[0]\n","\n","            normalized_window.index = np.arange(-window_before, window_after + 1)\n","            all_events.append(normalized_window)\n","\n","    except KeyError:\n","\n","        print(f\"Recession start date {start_date} not found in data index. Skipping.\")\n","        continue\n","\n","if not all_events:\n","    print(\"Could not generate event study plot: No recessions with a full historical window were found in the sample.\")\n","else:\n","    # Average the trajectories\n","    average_event_trajectory = pd.concat(all_events).groupby(level=0).mean()\n","\n","    fig, ax = plt.subplots(figsize=(14, 9))\n","\n","    special_vars = ['VIXCLSx', 'AAAFFM', 'BAAFFM']\n","    other_vars = [v for v in tcode1_vars if v not in special_vars]\n","\n","    average_event_trajectory[special_vars].plot(ax=ax, linewidth=3)\n","    average_event_trajectory[other_vars].plot(ax=ax, style='--', color='grey', linewidth=1.5, legend=False)\n","\n","    ax.axvline(x=0, color='red', linestyle='--', label='Recession Start (T=0)')\n","    ax.set_title('Average Behavior of Stationary Variables Around Recessions', fontsize=20)\n","    ax.set_xlabel('Months Relative to Recession Start', fontsize=12)\n","    ax.set_ylabel('Change from T-12 Months (Levels)', fontsize=12)\n","    ax.grid(True, linestyle='--', alpha=0.6)\n","    ax.legend()\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eoQ6IzqsaLX"},"outputs":[],"source":["# modeling interaction between real and fin to justify diffs for credit spreads\n","spread_vars = ['AAAFFM', 'BAAFFM', 'TB6SMFFM', 'T1YFFM', 'TB3SMFFM', 'COMPAPFFx', 'T5YFFM', 'T10YFFM']\n","\n","\n","in_sample_spreads = X_untransformed_full.loc[:'1989-12-31', spread_vars]\n","\n","\n","real_economy_indicator = np.log(X_untransformed_full['INDPRO']).diff().loc[:'1989-12-31']\n","\n","results = []\n","for var in spread_vars:\n","    level = in_sample_spreads[var]\n","    change = in_sample_spreads[var].diff()\n","\n","\n","    corr_level = pd.concat([real_economy_indicator, level], axis=1).dropna().corr().iloc[0, 1]\n","    corr_change = pd.concat([real_economy_indicator, change], axis=1).dropna().corr().iloc[0, 1]\n","\n","    # A high ratio means the \"change\" is more informative about the real economy\n","    if abs(corr_level) > 0:\n","        ratio = abs(corr_change) / abs(corr_level)\n","    else:\n","        ratio = np.nan\n","\n","    results.append({'Variable': var, 'Corr_Level': corr_level, 'Corr_Change': corr_change, 'Ratio': ratio})\n","\n","\n","results_df = pd.DataFrame(results).set_index('Variable')\n","print(\"Contagion Correlation Ratio (Change vs. Level with INDPRO Growth)\")\n","print(results_df.sort_values(by='Ratio', ascending=False))"]},{"cell_type":"markdown","metadata":{"id":"OC9ZkbBut8si"},"source":["## Predictor Set Generation"]},{"cell_type":"markdown","metadata":{"id":"XreyYELV2Qt3"},"source":["Includes functions for selecting top variables per category, then creating index (two factor, just weakness, just deterioration)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSNA-96GyhWN"},"outputs":[],"source":["lasso_weights_cache = {}\n","\n","def select_top_variables_per_category(X_data, y_data, variable_groups, horizon, top_n=10, corr_threshold=0.1):\n","    \"\"\"\n","    Selects the top N variables per category based on correlation.\n","    \"\"\"\n","    print(\"      -> Selecting top variables per category (Strict Top-N)...\")\n","\n","    y_shifted = y_data.shift(-horizon)\n","    y_shifted.name = 'y_lead'\n","\n","    aligned_data = pd.concat([y_shifted, X_data], axis=1).dropna()\n","    y_aligned = aligned_data['y_lead']\n","    X_aligned = aligned_data.drop(columns=['y_lead'])\n","\n","    if len(y_aligned.unique()) < 2:\n","        return {cat: vars[:top_n] for cat, vars in variable_groups.items()} # Fallback\n","\n","    refined_groups = {}\n","    for category, var_list in variable_groups.items():\n","        var_scores = {}\n","        for var in var_list:\n","            if var in X_aligned.columns:\n","                try:\n","                    correlation, _ = pointbiserialr(X_aligned[var], y_aligned)\n","                    if not np.isnan(correlation) and abs(correlation) >= corr_threshold:\n","                        var_scores[var] = abs(correlation)\n","                except ValueError:\n","                    continue\n","\n","        # Sort variables by score\n","        sorted_vars = sorted(var_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","\n","        # Extract only the names of the top N variables\n","        top_var_names = [var for var, score in sorted_vars[:top_n]]\n","\n","\n","        if top_var_names:\n","            refined_groups[category] = top_var_names # Store the list of names\n","\n","    return refined_groups\n","\n","\n","def get_final_definitive_span(X_transformed_data, variable_groups):\n","    \"\"\"\n","    Calculates the single, definitive, system-wide optimal span based on the\n","    median persistence of all variables actively used in the index\n","    \"\"\"\n","    print(\"Calculating the Final, Definitive System-Wide Optimal Span...\")\n","\n","    relevant_vars = [var for var_list in variable_groups.values() for var in var_list]\n","\n","    X_relevant = X_transformed_data[[v for v in relevant_vars if v in X_transformed_data.columns]]\n","\n","    # Calculate autocorrelation\n","    autocorrelations = X_relevant.apply(lambda s: s.autocorr(lag=1)).fillna(0).abs()\n","\n","    median_persistence = autocorrelations.median()\n","\n","    optimal_span = 1 / (median_persistence + 1e-9)\n","\n","    print(f\"\\n--- Definitive Results ---\")\n","    print(f\"Median Persistence of Relevant TRANSFORMED Signals: {median_persistence:.4f}\")\n","    print(f\"Calculated System-Wide Optimal Span: {optimal_span:.4f}\")\n","\n","    return optimal_span\n","\n","\n","def get_horizon_specific_optimal_span(X_transformed_data, y_target_data, horizon, variable_groups):\n","    \"\"\"\n","    Calculates the definitive optimal span for a specific horizon by focusing on the\n","    persistence of the most predictive variables at that horizon.\n","    \"\"\"\n","    print(f\"      -> Calculating Horizon-Specific Optimal Span for h={horizon}...\")\n","\n","    # 1. Identify the most predictive variables for THIS horizon\n","    y_shifted = y_target_data.shift(-horizon)\n","    relevant_vars = [var for var_list in variable_groups.values() for var in var_list if var in X_transformed_data.columns]\n","    X_relevant = X_transformed_data[relevant_vars]\n","\n","    data_for_corr = pd.concat([y_shifted, X_relevant], axis=1).dropna()\n","    predictive_power = data_for_corr.corr().iloc[0, 1:].abs()\n","\n","    # 2. Filter for the \"star players\" (above the median)\n","    median_power = predictive_power.median()\n","    star_players = predictive_power[predictive_power > median_power].index.tolist()\n","\n","    # 3. Calculate persistence of ONLY these star players\n","    X_star_players = X_relevant[star_players]\n","    autocorrelations = X_star_players.apply(lambda s: s.autocorr(lag=1)).fillna(0).abs()\n","\n","    # 4. Find the robust median persistence of this high-quality group\n","    median_persistence = autocorrelations.median()\n","\n","    # 5. Convert this to the final, optimal span for this horizon\n","    optimal_span = 1 / (median_persistence + 1e-9)\n","\n","    print(f\"         ... Median Persistence of Top Predictors for h={horizon}: {median_persistence:.4f}\")\n","    print(f\"         ... Calculated Optimal Span for h={horizon}: {optimal_span:.4f}\")\n","\n","    return optimal_span\n","\n","def generate_PCA_Factors(X_transformed_train, n_factors=8):\n","    \"\"\"\n","    Returns the top PCA factors.\n","    \"\"\"\n","    print(\"      -> Generating PCA Factors...\")\n","\n","    # Drop columns that are entirely NaN in the CURRENT training slice\n","    X_stat = X_transformed_train.copy()\n","    cols_to_drop_nan = X_stat.columns[X_stat.isna().all()]\n","    if not cols_to_drop_nan.empty:\n","        print(f\"\\n         ... Dropping {len(cols_to_drop_nan)} all-NaN columns: {cols_to_drop_nan.to_list()}\", end=\"\")\n","    X_stat_valid = X_stat.drop(columns=cols_to_drop_nan)\n","\n","\n","    # Imputation. Now guaranteed to have matching shapes\n","    imputer = KNNImputer(n_neighbors=5)\n","    X_imputed = pd.DataFrame(imputer.fit_transform(X_stat_valid),\n","                             index=X_stat_valid.index,\n","                             columns=X_stat_valid.columns)\n","\n","    # Standardization\n","    scaler = StandardScaler()\n","    X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed),\n","                            index=X_imputed.index,\n","                            columns=X_imputed.columns)\n","\n","    # Drop constant columns AFTER scaling (a final check)\n","    variances = X_scaled.var()\n","    constant_cols = variances[variances < 1e-10].index\n","    if not constant_cols.empty:\n","        print(f\"\\n         ... Dropping {len(constant_cols)} constant columns: {constant_cols.to_list()}\", end=\"\")\n","    X_final_for_pca = X_scaled.drop(columns=constant_cols)\n","\n","\n","    # PCA on the final, clean data\n","    pca = PCA(n_components=n_factors)\n","    factors = pca.fit_transform(X_final_for_pca)\n","\n","    pca_factors_df = pd.DataFrame(factors,\n","                                  index=X_final_for_pca.index,\n","                                  columns=[f'PCA_Factor_{i+1}' for i in range(n_factors)])\n","\n","    return pca_factors_df\n","\n","def generate_PCA_Factors_Binary(X_transformed_train, n_factors=8):\n","    \"\"\"\n","    Returns the top PCA factors.\n","    \"\"\"\n","    print(\"      -> Generating PCA Factors...\")\n","\n","    # Drop columns that are entirely NaN in the CURRENT training slice\n","    X_stat = X_transformed_train.copy()\n","    cols_to_drop_nan = X_stat.columns[X_stat.isna().all()]\n","    if not cols_to_drop_nan.empty:\n","        print(f\"\\n         ... Dropping {len(cols_to_drop_nan)} all-NaN columns: {cols_to_drop_nan.to_list()}\", end=\"\")\n","    X_stat_valid = X_stat.drop(columns=cols_to_drop_nan)\n","\n","    X_imputed = X_stat_valid.fillna(0) # missing value implies state of nondeterioration\n","\n","\n","    # Drop constant columns AFTER scaling (a final check)\n","    variances = X_imputed.var()\n","    constant_cols = variances[variances < 1e-10].index\n","    if not constant_cols.empty:\n","        print(f\"\\n         ... Dropping {len(constant_cols)} constant columns: {constant_cols.to_list()}\", end=\"\")\n","    X_final_for_pca = X_imputed.drop(columns=constant_cols)\n","\n","\n","    # PCA on the final, clean data\n","    pca = PCA(n_components=n_factors)\n","    factors = pca.fit_transform(X_final_for_pca)\n","\n","    pca_factors_df = pd.DataFrame(factors,\n","                                  index=X_final_for_pca.index,\n","                                  columns=[f'PCA_Factor_{i+1}' for i in range(n_factors)])\n","\n","    return pca_factors_df\n","\n","def generate_TFDI_Sub_Indices(X_transformed_train, y_train, horizon):\n","    \"\"\"\n","    This is the TDFI framework. It uses Ridge (L2) for nowcasting (h<3)\n","    to retain all signals, and LASSO (L1) for forecasting (h>=3) to perform\n","    automated feature selection and remove noise.\n","    \"\"\"\n","    print(f\"      -> Generating TFDI (h={horizon})...\")\n","\n","    # Generate Unweighted Sub-Indices\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM'],\n","        'FX_Rates': ['EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'AAAFFM', 'BAAFFM', 'VIXCLSx'}\n","    special_financial_vars = {'AAAFFM', 'BAAFFM', 'VIXCLSx'}\n","    X_momentum = X_transformed_train.rolling(window=3, min_periods=1).mean()\n","    # refined_variable_groups = select_top_variables_per_category(X_momentum, y_train, variable_groups, horizon=horizon, top_n=10)\n","    refined_variable_groups = variable_groups\n","\n","    adaptive_window = 180\n","    weakness_states = pd.DataFrame(index=X_transformed_train.index)\n","    deterioration_states = pd.DataFrame(index=X_transformed_train.index)\n","\n","\n","    all_selected_vars = [var for var_list in refined_variable_groups.values() for var in var_list]\n","    for var in all_selected_vars:\n","        signal_for_ranking = X_transformed_train[var]\n","        is_counter_theoretical = var in counter_cyclical_vars\n","        use_counter_logic = is_counter_theoretical\n","\n","        if horizon == 1 and var in special_financial_vars:\n","            use_counter_logic = True\n","            signal_for_ranking = X_transformed_train[var].diff()\n","        elif horizon > 1 and var in special_financial_vars:\n","            signal_for_ranking = X_transformed_train[var].diff()\n","\n","        level_signal = signal_for_ranking\n","        momentum_signal = signal_for_ranking.rolling(window=3, min_periods=1).mean()\n","\n","\n","        weakness_threshold = level_signal.quantile(0.8 if use_counter_logic else 0.2)\n","        weak_state = pd.Series(0.0, index=level_signal.index)\n","        if use_counter_logic: weak_state[level_signal > weakness_threshold] = 1.0\n","        else: weak_state[level_signal < weakness_threshold] = 1.0\n","        weakness_states[var] = weak_state\n","\n","        deterioration_threshold = momentum_signal.quantile(0.8 if use_counter_logic else 0.2)\n","        deteriorating_state = pd.Series(0.0, index=momentum_signal.index)\n","        if use_counter_logic: deteriorating_state[momentum_signal > deterioration_threshold] = 1.0\n","        else: deteriorating_state[momentum_signal < deterioration_threshold] = 1.0\n","        deterioration_states[var] = deteriorating_state\n","\n","\n","    # Aggregate into Unweighted Per-Category Sub-Indices\n","    cat_weakness_di = pd.DataFrame(index=X_transformed_train.index)\n","    cat_deterioration_di = pd.DataFrame(index=X_transformed_train.index)\n","\n","    for category, var_list in refined_variable_groups.items():\n","        # Only try to access columns that were actually selected\n","        weak_cols = [v for v in var_list if v in weakness_states.columns]\n","        if weak_cols:\n","            cat_weakness_di[f\"W_{category.replace(' ', '_')}\"] = weakness_states[weak_cols].mean(axis=1)\n","\n","        det_cols = [v for v in var_list if v in deterioration_states.columns]\n","        if det_cols:\n","            cat_deterioration_di[f\"D_{category.replace(' ', '_')}\"] = deterioration_states[det_cols].mean(axis=1)\n","\n","\n","    all_sub_indices = pd.concat([cat_weakness_di, cat_deterioration_di], axis=1)\n","    # Weighting step\n","    y_shifted = y_train.shift(-horizon).rename('y_lead')\n","    weighting_data = pd.concat([y_shifted, all_sub_indices], axis=1, join='inner').dropna()\n","    y_weight = weighting_data['y_lead']\n","    X_weight = weighting_data.drop(columns=['y_lead'])\n","\n","    weights = pd.Series(1.0, index=X_weight.columns) # Default to equal weights\n","\n","    if len(y_weight.unique()) == 2 and not X_weight.empty:\n","\n","\n","        if horizon < 3:\n","            print(\"         ... using LASSO (L1) weighting for forecasting.\")\n","            model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, C=0.1)\n","            param_grid = {'alpha': [0.1, 1.0, 10.0, 50.0, 100.0]}\n","        else:\n","            print(\"         ... using LASSO (L1) weighting for forecasting.\")\n","            model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, C=0.1)\n","            param_grid = {'C': [1.0, 0.5, 0.1, 0.05, 0.01]}\n","\n","        # Not fitting gridsearch because tuning does worse\n","        model.fit(X_weight, y_weight)\n","        weights = pd.Series(np.abs(model.coef_[0]), index=X_weight.columns)\n","\n","    return all_sub_indices.mul(weights).fillna(method='ffill').fillna(0)\n","\n","def generate_Weakness_Indices(X_transformed_train, y_train, horizon):\n","    \"\"\"\n","    This is the final, unified framework. It uses Ridge (L2) for nowcasting (h<3)\n","    to retain all signals, and LASSO (L1) for forecasting (h>=3) to perform\n","    automated feature selection and remove noise.\n","    \"\"\"\n","    print(f\"      -> Generating TFDI Weakness Indices (h={horizon})...\")\n","\n","\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM'],\n","        'FX_Rates': ['EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'AAAFFM', 'BAAFFM', 'VIXCLSx'}\n","    special_financial_vars = {'AAAFFM', 'BAAFFM', 'VIXCLSx'}\n","    X_momentum = X_transformed_train.rolling(window=12, min_periods=1).mean()\n","    # refined_variable_groups = select_top_variables_per_category(X_momentum, y_train, variable_groups, horizon=horizon, top_n=10)\n","    refined_variable_groups = variable_groups\n","    adaptive_window = 180\n","\n","    weakness_states = pd.DataFrame(index=X_transformed_train.index)\n","    deterioration_states = pd.DataFrame(index=X_transformed_train.index)\n","\n","    all_selected_vars = [var for var_list in refined_variable_groups.values() for var in var_list]\n","    for var in all_selected_vars:\n","        signal_for_ranking = X_transformed_train[var]\n","        is_counter_theoretical = var in counter_cyclical_vars\n","        use_counter_logic = is_counter_theoretical\n","\n","        if horizon == 1 and var in special_financial_vars:\n","            use_counter_logic = True\n","            signal_for_ranking = X_transformed_train[var].diff()\n","        elif horizon > 1 and var in special_financial_vars:\n","            signal_for_ranking = X_transformed_train[var].diff()\n","\n","        level_signal = signal_for_ranking\n","        momentum_signal = signal_for_ranking.rolling(window=3, min_periods=1).mean()\n","\n","\n","        weakness_threshold = level_signal.rolling(window=adaptive_window, min_periods=36).quantile(0.8 if use_counter_logic else 0.2)\n","        weak_state = pd.Series(0.0, index=level_signal.index)\n","        if use_counter_logic: weak_state[level_signal > weakness_threshold] = 1.0\n","        else: weak_state[level_signal < weakness_threshold] = 1.0\n","        weakness_states[var] = weak_state\n","\n","\n","    # Perform same operations as original function but only for Weakness\n","    cat_weakness_di = pd.DataFrame(index=X_transformed_train.index)\n","\n","    for category, var_list in refined_variable_groups.items():\n","\n","        weak_cols = [v for v in var_list if v in weakness_states.columns]\n","        if weak_cols:\n","            cat_weakness_di[f\"W_{category.replace(' ', '_')}\"] = weakness_states[weak_cols].mean(axis=1)\n","\n","\n","\n","    all_sub_indices = cat_weakness_di\n","\n","    y_shifted = y_train.shift(-horizon).rename('y_lead')\n","    weighting_data = pd.concat([y_shifted, all_sub_indices], axis=1, join='inner').dropna()\n","    y_weight = weighting_data['y_lead']\n","    X_weight = weighting_data.drop(columns=['y_lead'])\n","\n","    weights = pd.Series(1.0, index=X_weight.columns) # Default to equal weights\n","\n","    if len(y_weight.unique()) == 2 and not X_weight.empty:\n","\n","\n","\n","        if horizon < 3:\n","            print(\"         ... using LASSO (L2) weighting for forecasting.\")\n","            model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, C=0.1)\n","            param_grid = {'alpha': [0.1, 1.0, 10.0, 50.0, 100.0]}\n","        else:\n","            print(\"         ... using LASSO (L1) weighting for forecasting.\")\n","            model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, C=0.1)\n","            param_grid = {'C': [1.0, 0.5, 0.1, 0.05, 0.01]}\n","\n","        # Not fitting gridsearch because tuning does worse\n","        model.fit(X_weight, y_weight)\n","        weights = pd.Series(np.abs(model.coef_[0]), index=X_weight.columns)\n","\n","\n","    return all_sub_indices.mul(weights).fillna(method='ffill').fillna(0)\n","\n","\n","def generate_Deter_Indices(X_transformed_train, y_train, horizon, window=3):\n","    \"\"\"\n","    This is the final, unified framework. It uses Ridge (L2) for nowcasting (h<3)\n","    to retain all signals, and LASSO (L1) for forecasting (h>=3) to perform\n","    automated feature selection and remove noise.\n","    \"\"\"\n","    print(f\"      -> Generating Deterioration Indices (h={horizon})...\")\n","\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM', 'EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'VIXCLSx', 'BAAFFM', 'AAAFFM'}\n","    special_financial_vars = {'VIXCLSx', 'BAAFFM', 'AAAFFM'}\n","\n","    deterioration_states = pd.DataFrame(index=X_transformed_train.index)\n","    momentum_signals = pd.DataFrame(index=X_transformed_train.index)\n","\n","    all_selected_vars = [var for var_list in variable_groups.values() for var in var_list]\n","    for var in all_selected_vars:\n","        signal_for_ranking = X_transformed_train[var]\n","        is_counter_theoretical = var in counter_cyclical_vars\n","        use_counter_logic = is_counter_theoretical\n","\n","\n","        volatility = signal_for_ranking.std()\n","\n","        momentum = signal_for_ranking.rolling(window=horizon, min_periods=1).mean()\n","        input_signal = momentum\n","        # input_signal = signal_for_ranking.ewm(span=3, adjust=False).mean()\n","\n","        quantile = 0.25\n","\n","        counter_quantile = 1 - quantile\n","\n","        deterioration_threshold = input_signal.quantile(counter_quantile if use_counter_logic else quantile)\n","        deteriorating_state = pd.Series(0.0, index=input_signal.index)\n","        if use_counter_logic: deteriorating_state[input_signal > deterioration_threshold] = 1.0\n","        else: deteriorating_state[input_signal < deterioration_threshold] = 1.0\n","        deterioration_states[var] = deteriorating_state\n","\n","\n","\n","    cat_deterioration_di = pd.DataFrame(index=X_transformed_train.index)\n","\n","\n","    all_weights = {}\n","\n","\n","    for category, var_list in variable_groups.items():\n","\n","        det_cols = [v for v in var_list if v in deterioration_states.columns]\n","        signals_in_cat = deterioration_states[det_cols]\n","\n","\n","        # Weights are determined by corr * autocorr\n","        data_for_corr = pd.concat([y_train, signals_in_cat], axis=1).dropna()\n","\n","        if not data_for_corr.empty and data_for_corr.iloc[:, 0].nunique() > 1:\n","            predictive_power = data_for_corr.corr().iloc[0, 1:].abs()\n","        else:\n","            predictive_power = pd.Series(1.0, index=signals_in_cat.columns)\n","\n","        stability = signals_in_cat.apply(lambda s: s.autocorr(lag=1)).fillna(0).abs()\n","\n","        all_weights[category] = predictive_power * stability\n","\n","\n","    # Overall weight is average variable weight within cat\n","    category_importance_scores = {cat: weights.mean() for cat, weights in all_weights.items()}\n","\n","    all_weights_for_analysis = []\n","\n","    for category, var_list in variable_groups.items():\n","        det_cols = [v for v in var_list if v in deterioration_states.columns]\n","\n","        signals_in_cat = deterioration_states[det_cols]\n","        weights_in_cat = all_weights.get(category)\n","\n","        all_weights_for_analysis.append(weights_in_cat)\n","\n","        weighted_signals = signals_in_cat * weights_in_cat\n","        proportional_breadth_index = weighted_signals.mean(axis=1)\n","\n","        category_amplifier = category_importance_scores.get(category, 1.0)\n","\n","        # No amplifier\n","        final_index = proportional_breadth_index\n","\n","\n","        cat_deterioration_di[f\"D_{category.replace(' ', '_')}\"] = final_index\n","\n","\n","\n","    all_sub_indices = cat_deterioration_di.ffill().fillna(0)\n","\n","    final_weights_df = pd.concat(all_weights_for_analysis)\n","\n","\n","\n","    return all_sub_indices, final_weights_df, deterioration_states\n","\n","\n","def generate_Deter_Avg_Indices(X_transformed_train, y_train, horizon, window=3):\n","    \"\"\"\n","    This is the final, unified framework. It uses Ridge (L2) for nowcasting (h<3)\n","    to retain all signals, and LASSO (L1) for forecasting (h>=3) to perform\n","    automated feature selection and remove noise.\n","    \"\"\"\n","    print(f\"      -> Generating Deterioration Indices (h={horizon})...\")\n","\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM', 'EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'VIXCLSx', 'BAAFFM', 'AAAFFM'}\n","    special_financial_vars = {'VIXCLSx', 'BAAFFM', 'AAAFFM'}\n","\n","    deterioration_states = pd.DataFrame(index=X_transformed_train.index)\n","    momentum_signals = pd.DataFrame(index=X_transformed_train.index)\n","\n","    all_selected_vars = [var for var_list in variable_groups.values() for var in var_list]\n","    for var in all_selected_vars:\n","        signal_for_ranking = X_transformed_train[var]\n","        is_counter_theoretical = var in counter_cyclical_vars\n","        use_counter_logic = is_counter_theoretical\n","\n","\n","\n","\n","        momentum = signal_for_ranking.rolling(window=horizon, min_periods=1).median()\n","        input_signal = momentum\n","        # input_signal = signal_for_ranking.ewm(span=3, adjust=False).mean()\n","\n","\n","\n","        deterioration_threshold = input_signal.quantile(0.75 if use_counter_logic else 0.25)\n","        deteriorating_state = pd.Series(0.0, index=input_signal.index)\n","        if use_counter_logic: deteriorating_state[input_signal > deterioration_threshold] = 1.0\n","        else: deteriorating_state[input_signal < deterioration_threshold] = 1.0\n","        deterioration_states[var] = deteriorating_state\n","\n","    cat_deterioration_di = pd.DataFrame(index=X_transformed_train.index)\n","\n","\n","    for category, var_list in variable_groups.items():\n","        det_cols = [v for v in var_list if v in deterioration_states.columns]\n","        if det_cols:\n","            cat_deterioration_di[f\"D_{category.replace(' ', '_')}\"] = deterioration_states[det_cols].mean(axis=1)\n","\n","    all_sub_indices = cat_deterioration_di.ffill().fillna(0)\n","\n","\n","\n","    return all_sub_indices, deterioration_states\n","\n","\n","def add_lags(df, lags_to_add, prefix=''):\n","    \"\"\"\n","    Adds lagged versions of columns to a DataFrame.\n","    \"\"\"\n","    if not lags_to_add:\n","        return df\n","\n","    df_lagged = df.copy()\n","    for lag in lags_to_add:\n","        df_shifted = df.shift(lag)\n","        df_shifted.columns = [f'{prefix}{col}_lag{lag}' for col in df.columns]\n","        df_lagged = pd.concat([df_lagged, df_shifted], axis=1)\n","\n","    return df_lagged"]},{"cell_type":"markdown","source":["## C Optimization"],"metadata":{"id":"Yel9BEANFb3H"}},{"cell_type":"code","source":["# Dr. Shin's C selecting func\n","def optimize_C_value_first_sample_l1(X_train, y_train, C_values, n_splits=5):\n","    \"\"\"\n","    Optimize C value using time series cross-validation on the first training sample.\n","\n","    Parameters:\n","    -----------\n","    X_train : pd.DataFrame\n","        Training features\n","    y_train : pd.Series\n","        Training target\n","    C_values : array\n","        Grid of C values to test\n","    n_splits : int\n","        Number of time series splits for cross-validation (default: 7)\n","        With ~30 years of data, 7 folds gives ~4.3 years per validation set\n","\n","    Returns:\n","    --------\n","    best_C : float\n","        Best C value based on cross-validation\n","    \"\"\"\n","    if len(X_train) < 50:  # Need sufficient data for cross-validation\n","        print(\"      Insufficient data for C optimization, using default C=1.0\")\n","        return 1.0\n","\n","    tscv = TimeSeriesSplit(n_splits=n_splits)\n","    best_score = float('inf')\n","    best_C = 1.0\n","    c_scores = {}  # Store scores for each C value\n","\n","    print(f\"      Testing {len(C_values)} C values with {n_splits}-fold time series CV...\")\n","\n","    for C in C_values:\n","        scores = []\n","        try:\n","            for train_idx, val_idx in tscv.split(X_train):\n","                X_train_fold = X_train.iloc[train_idx]\n","                X_val_fold = X_train.iloc[val_idx]\n","                y_train_fold = y_train.iloc[train_idx]\n","                y_val_fold = y_train.iloc[val_idx]\n","\n","                # Fit model\n","                model = LogisticRegression(\n","                    penalty='l1',\n","                    solver='liblinear',\n","                    C=C,\n","                    max_iter=1000,\n","                    random_state=42,\n","                    class_weight='balanced'\n","                )\n","                model.fit(X_train_fold, y_train_fold)\n","\n","                # Predict and calculate Brier score\n","                y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n","                brier_score = np.mean((y_val_fold - y_pred_proba) ** 2)\n","                scores.append(brier_score)\n","\n","        except Exception as e:\n","            print(f\"      Error with C={C}: {e}\")\n","            continue\n","\n","        if scores:\n","            mean_score = np.mean(scores)\n","            c_scores[C] = mean_score\n","            if mean_score < best_score:\n","                best_score = mean_score\n","                best_C = C\n","\n","    print(f\"      Best C: {best_C:.6f} (Brier Score: {best_score:.6f})\")\n","    print(f\"      Tested {len(c_scores)} valid C values\")\n","\n","    # Show top 3 C values for reference\n","    if len(c_scores) > 3:\n","        sorted_c = sorted(c_scores.items(), key=lambda x: x[1])[:3]\n","        print(f\"      Top 3 C values: {[(c, score) for c, score in sorted_c]}\")\n","\n","    return best_C"],"metadata":{"id":"FQosY6uIFeW9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_C_value_first_sample_l2(X_train, y_train, C_values, n_splits=5):\n","    \"\"\"\n","    Optimize C value using time series cross-validation on the first training sample.\n","\n","    Parameters:\n","    -----------\n","    X_train : pd.DataFrame\n","        Training features\n","    y_train : pd.Series\n","        Training target\n","    C_values : array\n","        Grid of C values to test\n","    n_splits : int\n","        Number of time series splits for cross-validation (default: 7)\n","        With ~30 years of data, 7 folds gives ~4.3 years per validation set\n","\n","    Returns:\n","    --------\n","    best_C : float\n","        Best C value based on cross-validation\n","    \"\"\"\n","    if len(X_train) < 50:  # Need sufficient data for cross-validation\n","        print(\"      Insufficient data for C optimization, using default C=1.0\")\n","        return 1.0\n","\n","    tscv = TimeSeriesSplit(n_splits=n_splits)\n","    best_score = float('inf')\n","    best_C = 1.0\n","    c_scores = {}  # Store scores for each C value\n","\n","    print(f\"      Testing {len(C_values)} C values with {n_splits}-fold time series CV...\")\n","\n","    for C in C_values:\n","        scores = []\n","        try:\n","            for train_idx, val_idx in tscv.split(X_train):\n","                X_train_fold = X_train.iloc[train_idx]\n","                X_val_fold = X_train.iloc[val_idx]\n","                y_train_fold = y_train.iloc[train_idx]\n","                y_val_fold = y_train.iloc[val_idx]\n","\n","                # Fit model\n","                model = LogisticRegression(\n","                    penalty='l2',\n","                    solver='lbfgs',\n","                    C=C,\n","                    max_iter=1000,\n","                    random_state=42,\n","                    class_weight='balanced'\n","                )\n","                model.fit(X_train_fold, y_train_fold)\n","\n","                # Predict and calculate Brier score\n","                y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n","                brier_score = np.mean((y_val_fold - y_pred_proba) ** 2)\n","                scores.append(brier_score)\n","\n","        except Exception as e:\n","            print(f\"      Error with C={C}: {e}\")\n","            continue\n","\n","        if scores:\n","            mean_score = np.mean(scores)\n","            c_scores[C] = mean_score\n","            if mean_score < best_score:\n","                best_score = mean_score\n","                best_C = C\n","\n","    print(f\"      Best C: {best_C:.6f} (Brier Score: {best_score:.6f})\")\n","    print(f\"      Tested {len(c_scores)} valid C values\")\n","\n","    # Show top 3 C values for reference\n","    if len(c_scores) > 3:\n","        sorted_c = sorted(c_scores.items(), key=lambda x: x[1])[:3]\n","        print(f\"      Top 3 C values: {[(c, score) for c, score in sorted_c]}\")\n","\n","    return best_C"],"metadata":{"id":"nEU4k1UTX4Bp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qU2-P8evGvX"},"source":["## Main Forecasting Loop"]},{"cell_type":"markdown","metadata":{"id":"2Re5NJCz2ct5"},"source":["Parameters for the loop (such as horizons, lags, models, rerun control, etc.) can be found in the config cell."]},{"cell_type":"markdown","metadata":{"id":"jtdkK4mx3PD1"},"source":["To add a new predictor set:\n","\n","---\n","1. Create generator function in previous section.\n","2. Add to `ALL_POSSIBLE_SETS` array.\n","3. Add an `if` block in the predictor set generation section of the loop. Follow format of other sets.\n","4. Run the loop. It should detect that there is a new predictor set and only run that one, adding it to existing results (unless `FORCE_RERUN_ALL_SETS = True`).\n"]},{"cell_type":"markdown","metadata":{"id":"cd_NKIiL2xMU"},"source":["\n","\n","*   To rerun all sets, set `FORCE_RERUN_ALL_SETS` to `True`. This will ignore saved files and generate new results from scratch.\n","*   To rerun specific predictor set, add it to the `FORCE_RERUN_SPECIFIC_SETS` array.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1pVM-x92u-a"},"outputs":[],"source":["print(\"\\nStep 4: Starting recursive out-of-sample forecasting loop...\")\n","\n","SAVE_MODELS = True\n","OOS_MODELS_PATH = os.path.join(RESULTS_PATH, 'models')\n","\n","oos_deter_indices_history = []\n","\n","# Master Loop for All Horizons\n","for PREDICTION_HORIZON in PREDICTION_HORIZONS:\n","    print(f\"\\n{'='*25} Processing Horizon h={PREDICTION_HORIZON} {'='*25}\")\n","\n","    X_transformed_shifted = X_transformed_full.shift(PREDICTION_HORIZON)\n","    X_untransformed_shifted = X_untransformed_full.shift(PREDICTION_HORIZON)\n","    X_yield_shifted = X_yield_full.shift(PREDICTION_HORIZON)\n","    X_ads_shifted = X_ads_full.shift(PREDICTION_HORIZON)\n","\n","\n","    file_path = os.path.join(OOS_PRED_PATH, f'oos_results_h{PREDICTION_HORIZON}_hwindow.pkl')\n","\n","    horizon_sub_index_path = os.path.join(SUB_INDICES_PATH, f'h{PREDICTION_HORIZON}')\n","    os.makedirs(horizon_sub_index_path, exist_ok=True)\n","\n","    if SAVE_MODELS:\n","        horizon_model_path = os.path.join(OOS_MODELS_PATH, f'h{PREDICTION_HORIZON}')\n","        os.makedirs(horizon_model_path, exist_ok=True)\n","        print(f\"Models for h = {PREDICTION_HORIZON} will be saved to: {horizon_model_path}\")\n","\n","    # Load existing results or initialize new ones\n","    oos_probs, oos_errors, oos_actuals, oos_importances = {}, {}, None, {} # Start with empty dicts\n","    if not FORCE_RERUN_ALL_SETS:\n","        try:\n","            print(f\"Attempting to load existing results from: {file_path}\")\n","            oos_results = joblib.load(file_path)\n","            oos_probs = oos_results.get('probabilities', {}) # Use .get for safety\n","            oos_errors = oos_results.get('squared_errors', {})\n","            oos_actuals = oos_results.get('actuals', None)\n","            oos_importances = oos_results.get('importances', {})\n","            print(\"Successfully loaded existing results.\")\n","        except FileNotFoundError:\n","            print(\"No existing results file found. Initializing new structure.\")\n","    else:\n","        print(f\"FORCE_RERUN_ALL_SETS is True. Any loaded results will be ignored for this horizon.\")\n","\n","    # Determine which predictor sets need to be run\n","    ALL_POSSIBLE_SETS = ['Yield', 'Full', 'PCA_Factors_8', 'ADS', 'Deter', 'Deter_States', 'Deter_PCA', 'Deter_Avg']\n","\n","\n","    sets_to_run = []\n","    if FORCE_RERUN_ALL_SETS:\n","        sets_to_run = ALL_POSSIBLE_SETS\n","        print(f\"All {len(sets_to_run)} predictor sets will be re-run.\")\n","    else:\n","        for pred_set in ALL_POSSIBLE_SETS:\n","            # Rerun if not found or if specifically requested\n","            if pred_set not in oos_probs or pred_set in FORCE_RERUN_SPECIFIC_SETS:\n","                sets_to_run.append(pred_set)\n","        if FORCE_RERUN_SPECIFIC_SETS:\n","             print(f\"Will force rerun for specific sets: {FORCE_RERUN_SPECIFIC_SETS}\")\n","\n","    if not sets_to_run:\n","        print(\"All specified predictor sets have already been run for this horizon. Skipping.\")\n","        continue\n","\n","    print(f\"The following {len(sets_to_run)} sets will be run: {sets_to_run}\")\n","\n","    # Initialize/clear storage ONLY for the sets that are being run\n","    for pred_set in sets_to_run:\n","        oos_probs[pred_set] = {m: [] for m in MODELS_TO_RUN}\n","        oos_errors[pred_set] = {m: [] for m in MODELS_TO_RUN}\n","        oos_importances[pred_set] = {m: [] for m in MODELS_TO_RUN}\n","\n","\n","    target_col_name = 'USRECM'\n","\n","    # Main time-series loop\n","    all_dates = y_target_full.index\n","    forecast_dates = all_dates[all_dates >= pd.to_datetime(OOS_START_DATE)]\n","\n","    if oos_actuals is None or (len(oos_actuals) != len(forecast_dates)):\n","        oos_actuals = y_target_full.loc[forecast_dates, target_col_name]\n","        oos_actuals.index.name = 'Date'\n","\n","\n","    start_time = time.time()\n","\n","    optimal_C_values_l1 = {}\n","    optimal_C_values_l2 = {}\n","\n","    for i, forecast_date in enumerate(forecast_dates):\n","        iter_start_time = time.time()\n","        train_end_date = forecast_date - pd.DateOffset(months=PREDICTION_HORIZON)\n","        y_train_full = y_target_full.loc[:train_end_date, target_col_name]\n","        y_actual = oos_actuals.loc[forecast_date]\n","\n","        print(f\"Iter {i+1}/{len(forecast_dates)}: h={PREDICTION_HORIZON}, Date={forecast_date.date()}... \", end=\"\")\n","\n","        # Centralized Data Preparation\n","        X_train_transformed_slice = X_transformed_shifted.loc[:forecast_date].copy()\n","        X_untransformed_slice = X_untransformed_shifted.loc[:forecast_date].copy()\n","        X_train_valid_cols = X_train_transformed_slice.drop(columns=X_train_transformed_slice.columns[X_train_transformed_slice.isna().all()]).copy()\n","        imputer_base = KNNImputer(n_neighbors=5)\n","        X_train_imputed = pd.DataFrame(imputer_base.fit_transform(X_train_valid_cols), index=X_train_valid_cols.index, columns=X_train_valid_cols.columns)\n","        scaler = StandardScaler()\n","        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), index=X_train_imputed.index, columns=X_train_imputed.columns)\n","\n","\n","        # Generate Predictor Sets\n","        predictor_data_iter = {}\n","\n","        if 'ADS' in sets_to_run:\n","            predictor_data_iter['ADS'] = X_ads_shifted.loc[:forecast_date]\n","        if 'Yield' in sets_to_run:\n","            predictor_data_iter['Yield'] = X_yield_shifted.loc[:forecast_date]\n","        if 'Full' in sets_to_run: predictor_data_iter['Full'] = X_train_scaled\n","        if 'PCA_Factors_8' in sets_to_run:\n","            # The function does all preprocessing internally\n","            all_8_factors = generate_PCA_Factors(X_train_transformed_slice, n_factors=8)\n","            predictor_data_iter['PCA_Factors_8'] = all_8_factors\n","        if 'TFDI' in sets_to_run:\n","            predictor_data_iter['TFDI'] = generate_TFDI_Sub_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON)\n","        if 'Weakness' in sets_to_run:\n","            predictor_data_iter['Weakness'] = generate_Weakness_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON)\n","        if 'Deter' in sets_to_run:\n","            predictor_data_iter['Deter'], avg_weights, _ = generate_Deter_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON)\n","        if 'Deter_States' in sets_to_run:\n","            _, _, predictor_data_iter['Deter_States'] = generate_Deter_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON)\n","        if 'Deter_PCA' in sets_to_run:\n","            _, _, matrix = generate_Deter_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON)\n","            predictor_data_iter['Deter_PCA'] = generate_PCA_Factors(matrix, n_factors=8)\n","        if 'Deter_Avg' in sets_to_run:\n","            predictor_data_iter['Deter_Avg'], _ = generate_Deter_Avg_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON)\n","\n","        if i == 0:\n","            print(f\"\\n*** OPTIMIZING C VALUES ON FIRST TRAINING SAMPLE ***\")\n","            print(f\"Date: {forecast_date.date()}, Training data up to: {train_end_date.date()}\")\n","            print(f\"Using {len(C_values)} C values from {C_values[0]:.6f} to {C_values[-1]:.6f}\")\n","            print(f\"Will optimize C separately for each of the {len(sets_to_run)} predictor sets: {sets_to_run}\")\n","\n","            for pred_set_name in sets_to_run:\n","                print(f\"\\n  --- Processing {pred_set_name} ---\")\n","                X_train_raw = predictor_data_iter.get(pred_set_name)\n","\n","                if X_train_raw is None or X_train_raw.empty:\n","                    print(f\"    No data available, using default C=1.0\")\n","                    optimal_C_values_l1[pred_set_name] = 1.0\n","                    optimal_C_values_l2[pred_set_name] = 1.0\n","                    continue\n","\n","                # Add lags if specified\n","                X_train_lagged = add_lags(X_train_raw, LAGS_TO_ADD, prefix=f'{pred_set_name}_')\n","                X_train_lagged_final = X_train_lagged.dropna()\n","                X_train_final = X_train_lagged_final.loc[:train_end_date]\n","\n","                # Align with target\n","                common_index = y_train_full.index.intersection(X_train_final.index)\n","                y_train_aligned = y_train_full.loc[common_index]\n","                X_train_aligned = X_train_final.loc[common_index]\n","\n","                print(f\"    Data shape: {X_train_aligned.shape}, Target samples: {len(y_train_aligned)}\")\n","\n","                if len(X_train_aligned) > 50:  # Sufficient data for optimization\n","                    print(f\"    Optimizing C value using time series CV...\")\n","                    optimal_C_l1 = optimize_C_value_first_sample_l1(X_train_aligned, y_train_aligned, C_values)\n","                    optimal_C_values_l1[pred_set_name] = optimal_C_l1\n","\n","                    optimal_C_l2 = optimize_C_value_first_sample_l2(X_train_aligned, y_train_aligned, C_values)\n","                    optimal_C_values_l2[pred_set_name] = optimal_C_l2\n","                    print(f\"    Optimal C (L1) for {pred_set_name}: {optimal_C_l1:.6f}\")\n","                    print(f\"    Optimal C (L2) for {pred_set_name}: {optimal_C_l2:.6f}\")\n","                else:\n","                    print(f\"    Insufficient data ({len(X_train_aligned)} samples), using default C=1.0\")\n","                    optimal_C_values_l1[pred_set_name] = 1.0\n","                    optimal_C_values_l2[pred_set_name] = 1.0\n","\n","            print(f\"\\n*** C OPTIMIZATION COMPLETE ***\")\n","            print(\"Summary of optimal C (L1) values by predictor set:\")\n","            for pred_set, opt_c in optimal_C_values_l1.items():\n","                print(f\"  {pred_set:20s}: C = {opt_c:.6f}\")\n","            print(\"Summary of optimal C (L2) values by predictor set:\")\n","            for pred_set, opt_c in optimal_C_values_l2.items():\n","                print(f\"  {pred_set:20s}: C = {opt_c:.6f}\")\n","\n","\n","        # Loop over the INTENDED sets\n","        for pred_set_name in sets_to_run:\n","            X_train_raw = predictor_data_iter.get(pred_set_name)\n","\n","            if X_train_raw is None or X_train_raw.empty:\n","                for model_name in MODELS_TO_RUN:\n","                    oos_probs[pred_set_name][model_name].append(np.nan)\n","                    oos_errors[pred_set_name][model_name].append(np.nan)\n","                continue\n","            else:\n","                X_train_lagged = add_lags(X_train_raw, LAGS_TO_ADD, prefix=f'{pred_set_name}_')\n","\n","            X_train_lagged_final = X_train_lagged.dropna()\n","            X_train_final = X_train_lagged_final.loc[:train_end_date]\n","\n","            X_predict_point = X_train_lagged_final.loc[[forecast_date]]\n","\n","\n","            for model_name, model_template in MODELS_TO_RUN.items():\n","                prob, error = np.nan, np.nan\n","\n","                try:\n","                    common_index = y_train_full.index.intersection(X_train_final.index)\n","                    y_train_aligned = y_train_full.loc[common_index]\n","                    X_train_aligned = X_train_final.loc[common_index]\n","\n","                    if len(X_train_aligned) > max(LAGS_TO_ADD, default=0) + 20:\n","                        model_instance = clone(model_template)\n","                        params_to_set = {}\n","\n","                        if 'XGBoost' in model_name:\n","                            neg, pos = (y_train_aligned == 0).sum(), (y_train_aligned == 1).sum()\n","                            if pos > 0: params_to_set['scale_pos_weight'] = (neg / pos)\n","                        elif 'HGBoost' in model_name or 'RandomForest' in model_name or 'Logit' in model_name:\n","                            params_to_set['class_weight'] = 'balanced'\n","\n","                        if 'Logit_L1' in model_name and pred_set_name in optimal_C_values_l1:\n","                            params_to_set['C'] = optimal_C_values_l1[pred_set_name]\n","\n","\n","                        if 'Logit_L2' in model_name and pred_set_name in optimal_C_values_l2:\n","                            params_to_set['C'] = optimal_C_values_l2[pred_set_name]\n","\n","                        if params_to_set:\n","                            model_instance = model_instance.set_params(**params_to_set)\n","\n","                        X_predict_imputed = X_predict_point.reindex(columns=X_train_aligned.columns).ffill().bfill()\n","\n","                        if not X_predict_imputed.isna().any().any():\n","                            model_instance.fit(X_train_aligned, y_train_aligned)\n","\n","                            try:\n","                                if 'XGBoost' in model_name:\n","                                    booster = model_instance.get_booster()\n","                                    importance_dict = booster.get_score(importance_type='gain')\n","                                    importances = pd.Series(importance_dict, index=X_train_aligned.columns).fillna(0)\n","                                elif 'Logit' in model_name:\n","                                    importances = pd.Series(np.abs(model_instance.coef_[0]), index=X_train_aligned.columns)\n","                                else: # RandomForest, HGBoost\n","                                    importances = pd.Series(model_instance.feature_importances_, index=X_train_aligned.columns)\n","                            except Exception as e_imp:\n","                                print(f\"Could not get importances for {model_name} on {forecast_date.date()}: {e_imp}\")\n","                                pass\n","\n","                            if 'SAVE_MODELS' and forecast_date == forecast_dates[-1]:\n","                                print(\"Last iteration. Saving model...\")\n","                                model_filename = f'{pred_set_name}_{model_name}.pkl'\n","                                model_path = os.path.join(horizon_model_path, model_filename)\n","                                joblib.dump(model_instance, model_path)\n","\n","                            prob = model_instance.predict_proba(X_predict_imputed)[:, 1][0]\n","                            error = (y_actual - prob)**2\n","                except Exception as e:\n","                    print(f\"Error in model {model_name} for set {pred_set_name}: {e}\")\n","                    pass\n","\n","                oos_probs[pred_set_name][model_name].append(prob)\n","                oos_errors[pred_set_name][model_name].append(error)\n","                oos_importances[pred_set_name][model_name].append(importances)\n","\n","        iter_end_time = time.time()\n","        print(f\" ({(iter_end_time - iter_start_time):.2f}s)\")\n","\n","    # Save results after each horizon's loop is complete\n","    print(f\"\\n--- Loop for h={PREDICTION_HORIZON} Finished ---\")\n","    results_to_save = {'probabilities': oos_probs, 'squared_errors': oos_errors, 'actuals': oos_actuals, 'importances': oos_importances}\n","    joblib.dump(results_to_save, file_path)\n","    print(f\"Updated results saved to: {file_path}\")\n","\n","    print(\"=\" * 80)\n","    print(\"OPTIMAL C VALUES (SELECTED SEPARATELY FOR EACH PREDICTOR SET)\")\n","    print(\"=\" * 80)\n","    print(\"Each predictor set was optimized independently on the first training sample:\")\n","    print(f\"Total predictor sets: {len(optimal_C_values)}\")\n","    print(\"\")\n","    for pred_set_name, optimal_C in optimal_C_values.items():\n","        print(f\"{pred_set_name:25s}: C = {optimal_C:.6f}\")\n","    print(\"\")\n","    print(\"Benefits of set-specific C optimization:\")\n","    print(\"- Accounts for different feature dimensions across predictor sets\")\n","    print(\"- Adapts regularization to signal-to-noise ratio of each predictor type\")\n","    print(\"- Maximizes predictive performance while avoiding overfitting for each set\")\n","    print(\"\")\n","\n","# %% Report results\n","end_time = time.time()\n","print(f\"Total computation time: {end_time - start_time:.2f} seconds\")\n","print(\"\")\n","\n","print(\"\\n--- All Horizons Complete ---\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FQ4bXMaeUXQ"},"outputs":[],"source":["# weights for each variable, averaged over sample\n","avg_weights = avg_weights.groupby(level=0).mean()\n","top_20_vars = avg_weights.sort_values(ascending=False).head(20)\n","\n","# Create the bar plot\n","plt.figure(figsize=(10, 12))\n","top_20_vars.sort_values().plot(kind='barh')\n","plt.title('Top 20 Most Important Variables (Average Weight)', fontsize=16)\n","plt.xlabel('Average Dynamic Stability Weight')\n","plt.grid(axis='x', linestyle='--')\n","plt.show()"]},{"cell_type":"code","source":["def plot_signal_quality_diagnostic(X_data, y_data, var_name, horizon):\n","    \"\"\"\n","    Creates a diagnostic plot to visualize the momentum signal, the threshold,\n","    and the resulting Deter_State for a single variable and horizon.\n","    \"\"\"\n","    print(f\"--- Generating Signal Quality Diagnostic for '{var_name}' at h={horizon} ---\")\n","\n","    # --- Recreate the feature generation steps ---\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM', 'EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'VIXCLSx', 'BAAFFM', 'AAAFFM'}\n","\n","    # 1. Determine momentum window\n","    momentum_window = horizon\n","\n","    signal_for_ranking = X_data[var_name]\n","    input_signal = signal_for_ranking.rolling(window=momentum_window).mean()\n","\n","    # 3. Calculate the global quantile threshold\n","    #    This uses the \"full-sample estimation\" on the entire series for clarity\n","    is_counter_cyclical = var_name in counter_cyclical_vars # Assumes this is defined\n","    q = 0.75 if is_counter_cyclical else 0.25\n","    threshold = input_signal.quantile(q)\n","\n","    # 4. Create the binary state\n","    states = pd.Series(0.0, index=input_signal.index)\n","    if is_counter_cyclical:\n","        states[input_signal > threshold] = 1.0\n","    else:\n","        states[input_signal < threshold] = 1.0\n","\n","    # --- Create the Plot ---\n","    fig, ax = plt.subplots(figsize=(15, 7))\n","\n","    # Plot the momentum signal\n","    ax.plot(input_signal.index, input_signal, label=f'{momentum_window}-Month Momentum', color='green')\n","\n","    # Plot the threshold line\n","    ax.axhline(threshold, color='red', linestyle='--', label=f'{q*100:.0f}th Percentile Threshold = {threshold:.2f}')\n","\n","    # Plot the binary state on a secondary y-axis\n","    ax2 = ax.twinx()\n","    ax2.plot(states.index, states, label='Resulting Deter_State', color='blue', drawstyle='steps-post', linewidth=2)\n","\n","    # Add recession shading\n","    ax.fill_between(y_data.index, ax.get_ylim()[0], ax.get_ylim()[1],\n","                    where=y_data==1, color='gray', alpha=0.2,\n","                    transform=ax.get_xaxis_transform(), label='NBER Recession')\n","\n","    # Formatting\n","    ax.set_title(f'Signal Quality Diagnostic: {var_name} (h={horizon})', fontsize=16)\n","    ax.set_xlabel('Date')\n","    ax.set_ylabel('Momentum Value')\n","    ax2.set_ylabel('Binary State (0 or 1)', color='blue')\n","    ax2.tick_params(axis='y', labelcolor='blue')\n","    ax2.set_ylim(-0.1, 1.1)\n","\n","    # Combine legends\n","    lines, labels = ax.get_legend_handles_labels()\n","    lines2, labels2 = ax2.get_legend_handles_labels()\n","    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n","\n","    plt.show()\n","\n","# --- Run the Diagnostic ---\n","# Use the full historical data for this analysis\n","X_full_hist = X_transformed_full\n","y_full_hist = y_target_full['USRECM']\n","\n","# Run for the problematic horizon\n","plot_signal_quality_diagnostic(X_full_hist, y_full_hist, var_name='INDPRO', horizon=6)\n","plot_signal_quality_diagnostic(X_full_hist, y_full_hist, var_name='T10YFFM', horizon=6)\n","\n","# Run for a good horizon for comparison\n","plot_signal_quality_diagnostic(X_full_hist, y_full_hist, var_name='INDPRO', horizon=3)\n","plot_signal_quality_diagnostic(X_full_hist, y_full_hist, var_name='T10YFFM', horizon=12)"],"metadata":{"id":"X7KSXeazVKWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM', 'EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }"],"metadata":{"id":"GhdvFqacXv1Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","\n","def analyze_signal_lead_times(deterioration_states, y_true):\n","    \"\"\"\n","    Calculates the average lead time for each Deter_State signal relative to\n","    the start of NBER recessions and visualizes the results as a heatmap.\n","    \"\"\"\n","    print(\"--- Analyzing Average Signal Lead Times ---\")\n","\n","    # Identify the exact start dates of all historical recessions\n","    recession_starts = y_true[y_true.diff() == 1].index\n","\n","    lead_times = {}\n","\n","    # Loop through each signal (column) in the states matrix\n","    for signal_name in deterioration_states.columns:\n","        signal = deterioration_states[signal_name]\n","        signal_lead_times = []\n","\n","        # Loop through each historical recession\n","        for start_date in recession_starts:\n","            # Look in the 24 months prior to the recession for the first signal\n","            warning_window = signal.loc[start_date - pd.DateOffset(months=24):start_date]\n","\n","            # Find the first time the signal turned 'on' (flipped to 1) in that window\n","            first_signal_dates = warning_window[warning_window.diff() == 1].index\n","\n","            if not first_signal_dates.empty:\n","                first_signal_date = first_signal_dates.max() # Get the last flip to 'on'\n","                lead_time = (start_date - first_signal_date).days / 30.44 # Lead time in months\n","                signal_lead_times.append(lead_time)\n","\n","        # Calculate the average lead time for this signal across all recessions\n","        if signal_lead_times:\n","            lead_times[signal_name] = np.mean(signal_lead_times)\n","        else:\n","            lead_times[signal_name] = np.nan # No signal found for any recession\n","\n","    # --- Create the Heatmap ---\n","    lead_time_series = pd.Series(lead_times).dropna()\n","\n","    # Map each variable to its category\n","    feature_to_category = {var: cat for cat, var_list in variable_groups.items() for var in var_list}\n","    lead_time_df = lead_time_series.to_frame(name='AvgLeadTime')\n","    lead_time_df['Category'] = lead_time_df.index.map(feature_to_category)\n","\n","    # Pivot for the heatmap\n","    heatmap_data = lead_time_df.pivot_table(index='Category', columns=lead_time_df.index, values='AvgLeadTime')\n","\n","    plt.figure(figsize=(18, 8))\n","    sns.heatmap(heatmap_data, cmap='viridis', annot=False)\n","    plt.title('Average Lead Time of Deter_State Signals (h=12)', fontsize=16)\n","    plt.xlabel('Individual Indicator')\n","    plt.ylabel('Economic Category')\n","    plt.show()\n","\n","    return lead_time_df\n","\n","# --- How to Run It ---\n","# You need to generate the Deter_States for h=6 first\n","# This uses your horizon-specific momentum (window=6)\n","_, _, deter_states_h6 = generate_Deter_Indices(X_transformed_full, y_target_full['USRECM'], horizon=6)\n","\n","# Run the analysis\n","lead_time_results_h6 = analyze_signal_lead_times(deter_states_h6, y_target_full['USRECM'])\n","\n","# You can also run it for h=3 and h=12 to see the difference"],"metadata":{"id":"GrXE_D5cVihe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MEnpdkpmXzfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bvIcRhpCcJD0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"18T8ENtyOeY6oZ-3Jk9e__GduSEjCUfk2","authorship_tag":"ABX9TyMC7GdWXtrSo1BR54gtn9i9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}