{"cells":[{"cell_type":"markdown","metadata":{"id":"W-pSs-LMs1Qt"},"source":["## Configuration"]},{"cell_type":"markdown","metadata":{"id":"c7y2R6sgs4Qd"},"source":["This cell contains all key parameters for analysis. To replicate the paper's results, run this cell and \"Run All\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sRmu2MCx4Li"},"outputs":[],"source":["# Standard Imports\n","import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import joblib\n","\n","# Machine Learning and Preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.impute import KNNImputer\n","from sklearn.linear_model import LogisticRegression\n","import xgboost as xgb\n","from sklearn.ensemble import HistGradientBoostingClassifier as HGBoost\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.decomposition import PCA\n","from scipy.stats import pointbiserialr\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n","from sklearn.metrics import brier_score_loss, make_scorer, average_precision_score, log_loss, matthews_corrcoef, f1_score\n","from sklearn.base import clone\n","import statsmodels.api as sm\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import matplotlib.patches as mpatches\n","import shap\n","\n","\n","# Google Drive Mounting and Paths\n","from google.colab import drive\n","drive.mount('/content/drive')\n","BASE_PATH = \"/content/drive/MyDrive/Diffusion_Indices_Project/\"\n","INTERMEDIATE_PATH = os.path.join(BASE_PATH, \"03_intermediate_data\")\n","RESULTS_PATH = os.path.join(BASE_PATH, \"04_results\")\n","OOS_PRED_PATH = os.path.join(RESULTS_PATH, \"oos_predictions\")\n","SUB_INDICES_PATH = os.path.join(RESULTS_PATH, 'sub_indices_for_tuning')\n","VISUALS_PATH = os.path.join(BASE_PATH, '05_visuals')\n","os.makedirs(SUB_INDICES_PATH, exist_ok=True)\n","\n","# Create the results directories if they don't exist\n","os.makedirs(RESULTS_PATH, exist_ok=True)\n","os.makedirs(OOS_PRED_PATH, exist_ok=True)\n","\n","# Out-of-sample (OOS) Loop Settings\n","OOS_START_DATE = '1990-01-01'\n","PREDICTION_HORIZONS = [12]\n","USE_LAGS = False\n","\n","ngrid = 30\n","C_values = np.logspace(-3, 1, ngrid)\n","\n","# These are the five models used to generate the ensemble forecasts\n","MODELS_TO_RUN = {\n","    'Logit': LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000, random_state=42),\n","    'Logit_L1': LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42),\n","    'Logit_L2': LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42),\n","    # 'Logit_ElasticNet': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000),\n","    'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False),\n","}\n","\n","# Rerun Control\n","# Set to True to delete and regenerate all results files.\n","FORCE_RERUN_ALL_SETS = False\n","FORCE_RERUN_SPECIFIC_SETS = [] # This is ignored if the RERUN_ALL_SETS is True"]},{"cell_type":"markdown","metadata":{"id":"DP0GRbtAtzws"},"source":["## Loading Data"]},{"cell_type":"markdown","metadata":{"id":"sDLse4GRt2Oy"},"source":["The following steps load data from the original notebook that preprocesses data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXQtcmHmGnHi"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bKKdfXwyI4t"},"outputs":[],"source":["print(\"Loading analysis-ready datasets...\")\n","y_target_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'y_target.pkl'))\n","X_yield_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_yield.pkl'))\n","X_transformed_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_transformed_monthly.pkl'))\n","X_untransformed_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_untransformed_monthly.pkl'))\n","X_ads_full = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'X_ads.pkl'))\n","tcodes = pd.read_pickle(os.path.join(INTERMEDIATE_PATH, 'tcodes.pkl'))\n","\n","print(\"All data loaded successfully.\")\n","print(f\"Data shape: {X_transformed_full.shape}, Target shape: {y_target_full.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AB1K8yDb2oH"},"outputs":[],"source":["VARS_TO_REMOVE = ['ACOGNO', 'TWEXAFEGSMTHx', 'UMCSENTx', 'OILPRICEx']\n","\n","vars_that_exist_to_remove = [var for var in VARS_TO_REMOVE if var in X_transformed_full.columns]\n","\n","X_transformed_full = X_transformed_full.drop(columns=vars_that_exist_to_remove)\n","X_untransformed_full = X_untransformed_full.drop(columns=vars_that_exist_to_remove)\n","\n","print(f\"--- Data Filtering Complete ---\")\n","print(f\"Removed {len(vars_that_exist_to_remove)} problematic variables from all master DataFrames.\")\n","print(f\"The list of removed variables is: {vars_that_exist_to_remove}\")\n","print(f\"New data shape: {X_transformed_full.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"OC9ZkbBut8si"},"source":["## Predictor Set Generation"]},{"cell_type":"markdown","metadata":{"id":"XreyYELV2Qt3"},"source":["Includes functions for selecting top variables per category, then creating index (two factor, just weakness, just deterioration)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSNA-96GyhWN"},"outputs":[],"source":["def generate_PCA_Factors(X_transformed_train, n_factors=8):\n","    print(\"Generating PCA Factors...\")\n","\n","    X_stat = X_transformed_train.copy()\n","    cols_to_drop_nan = X_stat.columns[X_stat.isna().all()]\n","    if not cols_to_drop_nan.empty:\n","        print(f\"\\n Dropping {len(cols_to_drop_nan)} all-NaN columns: {cols_to_drop_nan.to_list()}\", end=\"\")\n","    X_stat_valid = X_stat.drop(columns=cols_to_drop_nan)\n","\n","    # Imputation on only current slice\n","    imputer = KNNImputer(n_neighbors=5)\n","    X_imputed = pd.DataFrame(imputer.fit_transform(X_stat_valid),\n","                             index=X_stat_valid.index,\n","                             columns=X_stat_valid.columns)\n","\n","    # Standardization\n","    scaler = StandardScaler()\n","    X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed),\n","                            index=X_imputed.index,\n","                            columns=X_imputed.columns)\n","\n","    # Drop constant columns\n","    variances = X_scaled.var()\n","    constant_cols = variances[variances < 1e-10].index\n","    if not constant_cols.empty:\n","        print(f\"\\n         ... Dropping {len(constant_cols)} constant columns: {constant_cols.to_list()}\", end=\"\")\n","    X_final_for_pca = X_scaled.drop(columns=constant_cols)\n","\n","\n","    pca = PCA(n_components=n_factors, random_state=42)\n","    factors = pca.fit_transform(X_final_for_pca)\n","\n","    pca_factors_df = pd.DataFrame(factors,\n","                                  index=X_final_for_pca.index,\n","                                  columns=[f'PCA_Factor_{i+1}' for i in range(n_factors)])\n","\n","    loadings = pca.components_.T\n","    loadings_df = pd.DataFrame(loadings,\n","                               index=X_final_for_pca.columns,\n","                               columns=[f'PCA_Factor_{i+1}' for i in range(n_factors)])\n","\n","    return pca_factors_df, loadings_df\n","\n","def generate_PCA_Factors_Binary(X_transformed_train, n_factors=8):\n","    print(\"Generating Binary PCA Factors...\")\n","\n","    X_stat = X_transformed_train.copy()\n","    cols_to_drop_nan = X_stat.columns[X_stat.isna().all()]\n","    if not cols_to_drop_nan.empty:\n","        print(f\"\\n Dropping {len(cols_to_drop_nan)} all-NaN columns: {cols_to_drop_nan.to_list()}\", end=\"\")\n","    X_stat_valid = X_stat.drop(columns=cols_to_drop_nan)\n","\n","    imputer = KNNImputer(n_neighbors=5)\n","    X_imputed = pd.DataFrame(imputer.fit_transform(X_stat_valid),\n","                             index=X_stat_valid.index,\n","                             columns=X_stat_valid.columns)\n","\n","    variances = X_imputed.var()\n","    constant_cols = variances[variances < 1e-10].index\n","    if not constant_cols.empty:\n","        print(f\"\\n Dropping {len(constant_cols)} constant columns: {constant_cols.to_list()}\", end=\"\")\n","    X_final_for_pca = X_imputed.drop(columns=constant_cols)\n","\n","\n","    pca = PCA(n_components=n_factors, random_state=42)\n","    factors = pca.fit_transform(X_final_for_pca)\n","\n","    pca_factors_df = pd.DataFrame(factors,\n","                                  index=X_final_for_pca.index,\n","                                  columns=[f'PCA_Factor_{i+1}' for i in range(n_factors)])\n","\n","    loadings = pca.components_.T\n","    loadings_df = pd.DataFrame(loadings,\n","                               index=X_final_for_pca.columns,\n","                               columns=[f'PCA_Factor_{i+1}' for i in range(n_factors)])\n","\n","    return pca_factors_df, loadings_df\n","\n","\n","\n","def generate_Deter_Indices(X_transformed_train, y_train, horizon, threshold=None, counter_threshold=None, var_threshold=None, sector_threshold=None, window_size=None):\n","    \"\"\"\n","    This is the final, unified framework. It uses Ridge (L2) for nowcasting (h<3)\n","    to retain all signals, and LASSO (L1) for forecasting (h>=3) to perform\n","    automated feature selection and remove noise.\n","    Accepts an optional data-driven counter_cyclical_vars list.\n","    \"\"\"\n","    print(f\"      -> Generating Deterioration Indices (h={horizon})...\")\n","\n","    variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM', 'EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","    }\n","    counter_cyclical_vars = {'UNRATE', 'UMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'ISRATIOx', 'VIXCLSx'}\n","\n","    deterioration_states = pd.DataFrame(index=X_transformed_train.index)\n","\n","\n","    all_selected_vars = [var for var_list in variable_groups.values() for var in var_list]\n","    for var in all_selected_vars:\n","        if var not in X_transformed_train.columns:\n","            continue\n","        signal_for_ranking = X_transformed_train[var]\n","        is_counter_theoretical = var in counter_cyclical_vars\n","        use_counter_logic = is_counter_theoretical\n","\n","        if window_size is not None:\n","            signal_for_ranking = signal_for_ranking.rolling(window=window_size, min_periods=1).mean()\n","        input_signal = signal_for_ranking\n","\n","        if threshold is not None:\n","            quantile = threshold\n","            if counter_threshold is not None:\n","                counter_quantile = counter_threshold\n","            else:\n","                counter_quantile = 1 - threshold\n","\n","        if var_threshold is not None:\n","            quantile = var_threshold.get(var)\n","            counter_quantile = 1 - quantile\n","\n","        if sector_threshold is not None:\n","            var_sector = None\n","            for sector, vars_list in variable_groups.items():\n","                if var in vars_list:\n","                    var_sector = sector\n","                    break\n","            quantile = sector_threshold.get(var_sector)\n","            counter_quantile = 1 - quantile\n","\n","        deterioration_threshold = input_signal.quantile(counter_quantile if use_counter_logic else quantile)\n","        deteriorating_state = pd.Series(0.0, index=input_signal.index)\n","        if use_counter_logic: deteriorating_state[input_signal > deterioration_threshold] = 1\n","        else: deteriorating_state[input_signal < deterioration_threshold] = 1\n","        deterioration_states[var] = deteriorating_state\n","\n","    return deterioration_states\n","\n","def add_lags(df, lags_to_add, prefix=''):\n","    \"\"\"\n","    Adds lagged versions of columns to a DataFrame.\n","    \"\"\"\n","    if not lags_to_add:\n","        return df\n","\n","    df_lagged = df.copy()\n","    for lag in lags_to_add:\n","        df_shifted = df.shift(lag)\n","        df_shifted.columns = [f'{prefix}{col}_lag{lag}' for col in df.columns]\n","        df_lagged = pd.concat([df_lagged, df_shifted], axis=1)\n","\n","    return df_lagged"]},{"cell_type":"markdown","metadata":{"id":"Yel9BEANFb3H"},"source":["## C Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQosY6uIFeW9"},"outputs":[],"source":["# Dr. Shin's C selecting func\n","def optimize_C_value_first_sample_l1(X_train, y_train, C_values, n_splits=5):\n","    \"\"\"\n","    Optimize C value using time series cross-validation on the first training sample.\n","\n","    Parameters:\n","    -----------\n","    X_train : pd.DataFrame\n","        Training features\n","    y_train : pd.Series\n","        Training target\n","    C_values : array\n","        Grid of C values to test\n","    n_splits : int\n","        Number of time series splits for cross-validation (default: 7)\n","        With ~30 years of data, 7 folds gives ~4.3 years per validation set\n","\n","    Returns:\n","    --------\n","    best_C : float\n","        Best C value based on cross-validation\n","    \"\"\"\n","    if len(X_train) < 50:  # Need sufficient data for cross-validation\n","        print(\"      Insufficient data for C optimization, using default C=1.0\")\n","        return 1.0\n","\n","    tscv = TimeSeriesSplit(n_splits=n_splits)\n","    best_score = float('inf')\n","    best_C = 1.0\n","    c_scores = {}  # Store scores for each C value\n","\n","    print(f\"      Testing {len(C_values)} C values with {n_splits}-fold time series CV...\")\n","\n","    for C in C_values:\n","        scores = []\n","        try:\n","            for train_idx, val_idx in tscv.split(X_train):\n","                X_train_fold = X_train.iloc[train_idx]\n","                X_val_fold = X_train.iloc[val_idx]\n","                y_train_fold = y_train.iloc[train_idx]\n","                y_val_fold = y_train.iloc[val_idx]\n","\n","                # Fit model\n","                model = LogisticRegression(\n","                    penalty='l1',\n","                    solver='liblinear',\n","                    C=C,\n","                    max_iter=1000,\n","                    random_state=42,\n","                    class_weight='balanced'\n","                )\n","                model.fit(X_train_fold, y_train_fold)\n","\n","                # Predict and calculate Brier score\n","                y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n","                brier_score = np.mean((y_val_fold - y_pred_proba) ** 2)\n","                scores.append(brier_score)\n","\n","        except Exception as e:\n","            print(f\"      Error with C={C}: {e}\")\n","            continue\n","\n","        if scores:\n","            mean_score = np.mean(scores)\n","            c_scores[C] = mean_score\n","            if mean_score < best_score:\n","                best_score = mean_score\n","                best_C = C\n","\n","    print(f\"      Best C: {best_C:.6f} (Brier Score: {best_score:.6f})\")\n","    print(f\"      Tested {len(c_scores)} valid C values\")\n","\n","    # Show top 3 C values for reference\n","    if len(c_scores) > 3:\n","        sorted_c = sorted(c_scores.items(), key=lambda x: x[1])[:3]\n","        print(f\"      Top 3 C values: {[(c, score) for c, score in sorted_c]}\")\n","\n","    return best_C"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEU4k1UTX4Bp"},"outputs":[],"source":["def optimize_C_value_first_sample_l2(X_train, y_train, C_values, n_splits=5):\n","    \"\"\"\n","    Optimize C value using time series cross-validation on the first training sample.\n","\n","    Parameters:\n","    -----------\n","    X_train : pd.DataFrame\n","        Training features\n","    y_train : pd.Series\n","        Training target\n","    C_values : array\n","        Grid of C values to test\n","    n_splits : int\n","        Number of time series splits for cross-validation (default: 7)\n","        With ~30 years of data, 7 folds gives ~4.3 years per validation set\n","\n","    Returns:\n","    --------\n","    best_C : float\n","        Best C value based on cross-validation\n","    \"\"\"\n","    if len(X_train) < 50:  # Need sufficient data for cross-validation\n","        print(\"      Insufficient data for C optimization, using default C=1.0\")\n","        return 1.0\n","\n","    tscv = TimeSeriesSplit(n_splits=n_splits)\n","    best_score = float('inf')\n","    best_C = 1.0\n","    c_scores = {}  # Store scores for each C value\n","\n","    print(f\"      Testing {len(C_values)} C values with {n_splits}-fold time series CV...\")\n","\n","    for C in C_values:\n","        scores = []\n","        try:\n","            for train_idx, val_idx in tscv.split(X_train):\n","                X_train_fold = X_train.iloc[train_idx]\n","                X_val_fold = X_train.iloc[val_idx]\n","                y_train_fold = y_train.iloc[train_idx]\n","                y_val_fold = y_train.iloc[val_idx]\n","\n","                # Fit model\n","                model = LogisticRegression(\n","                    penalty='l2',\n","                    solver='lbfgs',\n","                    C=C,\n","                    max_iter=1000,\n","                    random_state=42,\n","                    class_weight='balanced'\n","                )\n","                model.fit(X_train_fold, y_train_fold)\n","\n","                # Predict and calculate Brier score\n","                y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n","                brier_score = np.mean((y_val_fold - y_pred_proba) ** 2)\n","                scores.append(brier_score)\n","\n","        except Exception as e:\n","            print(f\"      Error with C={C}: {e}\")\n","            continue\n","\n","        if scores:\n","            mean_score = np.mean(scores)\n","            c_scores[C] = mean_score\n","            if mean_score < best_score:\n","                best_score = mean_score\n","                best_C = C\n","\n","    print(f\"      Best C: {best_C:.6f} (Brier Score: {best_score:.6f})\")\n","    print(f\"      Tested {len(c_scores)} valid C values\")\n","\n","    # Show top 3 C values for reference\n","    if len(c_scores) > 3:\n","        sorted_c = sorted(c_scores.items(), key=lambda x: x[1])[:3]\n","        print(f\"      Top 3 C values: {[(c, score) for c, score in sorted_c]}\")\n","\n","    return best_C"]},{"cell_type":"markdown","metadata":{"id":"DIwnHK4EpfiU"},"source":["## Optimal Threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQNCkN6tboUn"},"outputs":[],"source":["def get_empirical_unified_quantile(X_in_sample, y_in_sample, counter_cyclical_vars, horizon, momentum_window):\n","\n","    print(f\"\\nDeriving the Empirical Unified Quantile Threshold for h={horizon}\")\n","\n","    if isinstance(y_in_sample, pd.DataFrame):\n","        y_series = y_in_sample.iloc[:, 0]\n","    else:\n","        y_series = y_in_sample\n","\n","    X_unified = X_in_sample.copy()\n","    for var in counter_cyclical_vars:\n","        if var in X_unified.columns:\n","            X_unified[var] = X_unified[var] * -1\n","\n","    recession_periods = y_series[y_series == 1].index\n","\n","    median_recessionary_quantiles = []\n","\n","    for var in X_unified.columns:\n","        momentum = X_unified[var].rolling(window=momentum_window).mean()\n","\n","        rank = momentum.rank(pct=True)\n","\n","        ranks_during_recession = rank.loc[rank.index.intersection(recession_periods)].dropna()\n","\n","        if not ranks_during_recession.empty:\n","            median_rank = ranks_during_recession.median()\n","            median_recessionary_quantiles.append(median_rank)\n","\n","    final_empirical_quantile = np.median(median_recessionary_quantiles)\n","\n","    print(f\"Empirically Derived Consensus Quantile Threshold: {final_empirical_quantile:.3f}\")\n","\n","    return final_empirical_quantile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwHd2F1dF0TL"},"outputs":[],"source":["def get_empirical_variable_quantiles(X_in_sample, y_in_sample, counter_cyclical_vars, horizon):\n","    \"\"\"\n","    Derives the empirical quantile threshold for each individual variable based on its median rank during recessions.\n","    \"\"\"\n","\n","    if isinstance(y_in_sample, pd.DataFrame):\n","        y_series = y_in_sample.iloc[:, 0]\n","    else:\n","        y_series = y_in_sample\n","\n","    X_unified = X_in_sample.copy()\n","    for var in counter_cyclical_vars:\n","        if var in X_unified.columns:\n","            X_unified[var] = X_unified[var] * -1\n","\n","    X_ranks = X_unified.rank(pct=True)\n","\n","    recession_periods = y_in_sample[y_in_sample == 1].index\n","\n","    ranks_during_recession = X_ranks.loc[X_ranks.index.intersection(recession_periods)].dropna()\n","\n","    # Calculate the median rank for each variable during recessions\n","    median_variable_ranks = ranks_during_recession.median()\n","\n","    variable_quantiles = median_variable_ranks.to_dict()\n","\n","    print(f\"Derived empirical quantiles for {len(variable_quantiles)} variables.\")\n","\n","    return variable_quantiles"]},{"cell_type":"code","source":["def get_empirical_sector_quantiles(X_in_sample, y_in_sample, variable_groups, counter_cyclical_vars, horizon):\n","    \"\"\"\n","    Derives the empirical quantile threshold for each individual variable based on its median rank during recessions.\n","    \"\"\"\n","\n","    if isinstance(y_in_sample, pd.DataFrame):\n","        y_series = y_in_sample.iloc[:, 0]\n","    else:\n","        y_series = y_in_sample\n","\n","    X_unified = X_in_sample.copy()\n","    for var in counter_cyclical_vars:\n","        if var in X_unified.columns:\n","            X_unified[var] = X_unified[var] * -1\n","\n","    X_ranks = X_unified.rank(pct=True)\n","\n","    recession_periods = y_in_sample[y_in_sample == 1].index\n","\n","    ranks_during_recession = X_ranks.loc[X_ranks.index.intersection(recession_periods)].dropna()\n","\n","    # Calculate the median rank for each variable during recessions\n","    median_variable_ranks = ranks_during_recession.median()\n","\n","    sector_quantiles = {}\n","    for sector, var_list in variable_groups.items():\n","        vars_in_sector = [var for var in var_list if var in median_variable_ranks.index]\n","        if vars_in_sector:\n","          sector_quantiles[sector] = median_variable_ranks.loc[vars_in_sector].median()\n","\n","    return sector_quantiles"],"metadata":{"id":"9_b25X-0FNoQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Variables"],"metadata":{"id":"5US73vHYF8IT"}},{"cell_type":"code","source":["variable_groups = {\n","        'Output_Income': ['RPI', 'W875RX1', 'INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD', 'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S', 'IPFUELS', 'CUMFNS'],\n","        'Labor_Market': ['HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV', 'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV', 'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD', 'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP', 'SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT', 'CES0600000007', 'AWOTMAN', 'AWHMAN', 'CES0600000008', 'CES2000000008', 'CES3000000008'],\n","        'Housing': ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW'],\n","        'Consumption_Orders_Inventories': ['DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx', 'AMDMNOx', 'AMDMUOx', 'ANDENOx', 'BUSINVx', 'ISRATIOx'],\n","        'Money_Credit': ['M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS', 'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'DTCOLNVHFNM', 'DTCTHFNM', 'INVEST'],\n","        'Interest_Rates_Spreads': ['FEDFUNDS', 'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA', 'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM', 'AAAFFM', 'BAAFFM', 'EXSZUSx', 'EXJPUSx', 'EXUSUKx', 'EXCAUSx'],\n","        'Prices': ['WPSFD49207', 'WPSFD49502', 'WPSID61', 'WPSID62', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL', 'CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD', 'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5', 'PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA'],\n","        'Stock_Market': ['S&P 500', 'S&P div yield', 'S&P PE ratio', 'VIXCLSx']\n","}"],"metadata":{"id":"nV_mM0mPF9-v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qU2-P8evGvX"},"source":["## Main Forecasting Loop"]},{"cell_type":"markdown","metadata":{"id":"2Re5NJCz2ct5"},"source":["Parameters for the loop (such as horizons, lags, models, rerun control, etc.) can be found in the config cell."]},{"cell_type":"markdown","metadata":{"id":"jtdkK4mx3PD1"},"source":["To add a new predictor set:\n","\n","---\n","1. Create generator function in previous section.\n","2. Add to `ALL_POSSIBLE_SETS` array.\n","3. Add an `if` block in the predictor set generation section of the loop. Follow format of other sets.\n","4. Run the loop. It should detect that there is a new predictor set and only run that one, adding it to existing results (unless `FORCE_RERUN_ALL_SETS = True`).\n"]},{"cell_type":"markdown","metadata":{"id":"cd_NKIiL2xMU"},"source":["\n","\n","*   To rerun all sets, set `FORCE_RERUN_ALL_SETS` to `True`. This will ignore saved files and generate new results from scratch.\n","*   To rerun specific predictor set, add it to the `FORCE_RERUN_SPECIFIC_SETS` array.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1pVM-x92u-a"},"outputs":[],"source":["print(\"\\nStep 4: Starting recursive out-of-sample forecasting loop...\")\n","\n","SAVE_MODELS = True\n","OOS_MODELS_PATH = os.path.join(RESULTS_PATH, 'models')\n","\n","oos_deter_indices_history = []\n","\n","oos_loadings = {}\n","\n","unified_thresholds = {}\n","separate_thresholds = {}\n","counter_cyclical_vars = {\n","    'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV',\n","    'UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'VIXCLSx', 'ISRATIOx'\n","}\n","\n","USE_SUBSETS = False\n","subset = ['PAYEMS', 'CLAIMSx', 'UNRATE', 'AWHMAN', 'INDPRO', 'RPI', 'HOUST', 'RETAILx']\n","\n","\n","# Master Loop for All Horizons\n","for PREDICTION_HORIZON in PREDICTION_HORIZONS:\n","    print(f\"\\n{'='*25} Processing Horizon h={PREDICTION_HORIZON} {'='*25}\")\n","\n","    X_transformed_shifted = X_transformed_full.shift(PREDICTION_HORIZON)\n","    X_untransformed_shifted = X_untransformed_full.shift(PREDICTION_HORIZON)\n","    X_yield_shifted = X_yield_full.shift(PREDICTION_HORIZON)\n","    X_ads_shifted = X_ads_full.shift(PREDICTION_HORIZON)\n","\n","    if USE_LAGS:\n","        LAGS_TO_ADD = [3, 6, 12]\n","    else:\n","        LAGS_TO_ADD = []\n","\n","    if LAGS_TO_ADD:\n","      file_path = os.path.join(OOS_PRED_PATH, f'oos_results_h{PREDICTION_HORIZON}_final_fixlagged_bestpred.pkl')\n","    else:\n","      file_path = os.path.join(OOS_PRED_PATH, f'oos_results_h{PREDICTION_HORIZON}_final_marx_maf.pkl')\n","\n","    horizon_sub_index_path = os.path.join(SUB_INDICES_PATH, f'h{PREDICTION_HORIZON}')\n","    os.makedirs(horizon_sub_index_path, exist_ok=True)\n","\n","    if SAVE_MODELS:\n","        horizon_model_path = os.path.join(OOS_MODELS_PATH, f'h{PREDICTION_HORIZON}')\n","        os.makedirs(horizon_model_path, exist_ok=True)\n","        print(f\"Models for h = {PREDICTION_HORIZON} will be saved to: {horizon_model_path}\")\n","\n","\n","    oos_loadings[PREDICTION_HORIZON] = {\n","        'PCA_Factors_8': {},\n","        'Deter_PCA': {},\n","        'Deter_PCA_SecQ': {},\n","        'Deter_PCA_VarQ': {},\n","    }\n","    # Load existing results or initialize new ones\n","    oos_probs, oos_errors, oos_actuals, oos_importances = {}, {}, None, {} # Start with empty dicts\n","    if not FORCE_RERUN_ALL_SETS:\n","        try:\n","            print(f\"Attempting to load existing results from: {file_path}\")\n","            oos_results = joblib.load(file_path)\n","            oos_probs = oos_results.get('probabilities', {}) # Use .get for safety\n","            oos_errors = oos_results.get('squared_errors', {})\n","            oos_actuals = oos_results.get('actuals', None)\n","            oos_importances = oos_results.get('importances', {})\n","            print(\"Successfully loaded existing results.\")\n","        except FileNotFoundError:\n","            print(\"No existing results file found. Initializing new structure.\")\n","    else:\n","        print(f\"FORCE_RERUN_ALL_SETS is True. Any loaded results will be ignored for this horizon.\")\n","\n","    # Determine which predictor sets need to be run\n","    ALL_POSSIBLE_SETS = ['MARX', 'MAF']\n","\n","\n","    sets_to_run = []\n","    if FORCE_RERUN_ALL_SETS:\n","        sets_to_run = ALL_POSSIBLE_SETS\n","        print(f\"All {len(sets_to_run)} predictor sets will be re-run.\")\n","    else:\n","        for pred_set in ALL_POSSIBLE_SETS:\n","            # Rerun if not found or if specifically requested\n","            if pred_set not in oos_probs or pred_set in FORCE_RERUN_SPECIFIC_SETS:\n","                sets_to_run.append(pred_set)\n","        if FORCE_RERUN_SPECIFIC_SETS:\n","             print(f\"Will force rerun for specific sets: {FORCE_RERUN_SPECIFIC_SETS}\")\n","\n","    if not sets_to_run:\n","        print(\"All specified predictor sets have already been run for this horizon. Skipping.\")\n","        continue\n","\n","    print(f\"The following {len(sets_to_run)} sets will be run: {sets_to_run}\")\n","\n","    # Initialize/clear storage ONLY for the sets that are being run\n","    for pred_set in sets_to_run:\n","        oos_probs[pred_set] = {m: [] for m in MODELS_TO_RUN}\n","        oos_errors[pred_set] = {m: [] for m in MODELS_TO_RUN}\n","        oos_importances[pred_set] = {m: [] for m in MODELS_TO_RUN}\n","\n","\n","    target_col_name = 'USRECM'\n","\n","    # Main time-series loop\n","    all_dates = y_target_full.index\n","    forecast_dates = all_dates[all_dates >= pd.to_datetime(OOS_START_DATE)]\n","\n","    if oos_actuals is None or (len(oos_actuals) != len(forecast_dates)):\n","        oos_actuals = y_target_full.loc[forecast_dates, target_col_name]\n","        oos_actuals.index.name = 'Date'\n","\n","\n","    start_time = time.time()\n","\n","    optimal_C_values_l1 = {}\n","    optimal_C_values_l2 = {}\n","\n","    oos_training_datasets = {pred_set: [] for pred_set in sets_to_run}\n","\n","\n","    for i, forecast_date in enumerate(forecast_dates):\n","        iter_start_time = time.time()\n","        train_end_date = forecast_date - pd.DateOffset(months=PREDICTION_HORIZON)\n","\n","        y_train_full = y_target_full.loc[:train_end_date, target_col_name]\n","        y_actual = oos_actuals.loc[forecast_date]\n","\n","        print(f\"Iter {i+1}/{len(forecast_dates)}: h={PREDICTION_HORIZON}, Date={forecast_date.date()}... \", end=\"\")\n","\n","\n","        # Centralized Data Preparation\n","        X_train_transformed_slice = X_transformed_shifted.loc[:forecast_date].copy()\n","        if USE_SUBSETS:\n","            X_train_transformed_slice = X_train_transformed_slice[subset]\n","        X_untransformed_slice = X_untransformed_shifted.loc[:forecast_date].copy()\n","\n","        X_train_valid_cols = X_train_transformed_slice.drop(columns=X_train_transformed_slice.columns[X_untransformed_slice.isna().all()]).copy()\n","        imputer_base = KNNImputer(n_neighbors=5)\n","        X_train_imputed = pd.DataFrame(imputer_base.fit_transform(X_train_valid_cols), index=X_train_valid_cols.index, columns=X_train_valid_cols.columns)\n","        scaler = StandardScaler()\n","        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), index=X_train_imputed.index, columns=X_train_imputed.columns)\n","\n","        # Quantile threshold tuning\n","        first_forecast_date = pd.to_datetime(OOS_START_DATE)\n","        tuning_train_end_date = first_forecast_date - pd.DateOffset(months=PREDICTION_HORIZON)\n","\n","        X_in_sample_for_tuning = X_transformed_full.loc[:tuning_train_end_date]\n","        if USE_SUBSETS:\n","            X_in_sample_for_tuning = X_in_sample_for_tuning[subset]\n","        y_in_sample_for_tuning = y_target_full.loc[:tuning_train_end_date, 'USRECM']\n","\n","        optimal_q = get_empirical_unified_quantile(X_in_sample_for_tuning, y_in_sample_for_tuning, counter_cyclical_vars, horizon=PREDICTION_HORIZON, momentum_window=12)\n","        optimal_var_q = get_empirical_variable_quantiles(X_in_sample_for_tuning, y_in_sample_for_tuning, counter_cyclical_vars, horizon=PREDICTION_HORIZON)\n","        optimal_sec_q = get_empirical_sector_quantiles(X_in_sample_for_tuning, y_in_sample_for_tuning, variable_groups, counter_cyclical_vars, horizon=PREDICTION_HORIZON)\n","\n","\n","        if i == 0:\n","          print(f\"Unified Quantile Threshold for h={PREDICTION_HORIZON}: {optimal_q}\")\n","          print(\"=\"*70 + \"\\n\")\n","          print(f\"Variable Quantile Thresholds for h={PREDICTION_HORIZON}: {optimal_var_q}\")\n","          print(\"=\"*70 + \"\\n\")\n","          print(f\"Sector Quantile Thresholds for h={PREDICTION_HORIZON}: {optimal_sec_q}\")\n","          print(\"=\"*70 + \"\\n\")\n","\n","        # Generate Predictor Sets\n","        predictor_data_iter = {}\n","\n","\n","        # if 'ADS' in sets_to_run:\n","        #     predictor_data_iter['ADS'] = X_ads_shifted.loc[:forecast_date]\n","        # if 'Yield' in sets_to_run:\n","        #     predictor_data_iter['Yield'] = X_yield_shifted.loc[:forecast_date]\n","        if 'Full' in sets_to_run: predictor_data_iter['Full'] = X_train_scaled\n","        if 'PCA_Factors_8' in sets_to_run:\n","            all_8_factors, loadings = generate_PCA_Factors(X_train_transformed_slice, n_factors=8)\n","            predictor_data_iter['PCA_Factors_8'] = all_8_factors\n","            oos_loadings[PREDICTION_HORIZON]['PCA_Factors_8'][forecast_date] = loadings\n","\n","        states_global_q = None\n","        if any(s in ['Deter_States', 'Deter_PCA', 'Deter_Avg', 'Deter_States_BestPred'] for s in sets_to_run):\n","            print(\"Generating binary states with GLOBAL threshold...\")\n","            states_global_q = generate_Deter_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON, threshold=optimal_q)\n","\n","        states_sector_q = None\n","        if any(s in ['Deter_States_SecQ', 'Deter_PCA_SecQ', 'Deter_Avg_SecQ'] for s in sets_to_run):\n","            print(\"Generating binary states with SECTOR thresholds...\")\n","            states_sector_q = generate_Deter_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON, sector_threshold=optimal_sec_q)\n","\n","        states_variable_q = None\n","        if any(s in ['Deter_States_VarQ', 'Deter_PCA_VarQ', 'Deter_Avg_VarQ'] for s in sets_to_run):\n","            print(\"Generating binary states with VARIABLE thresholds...\")\n","            states_variable_q = generate_Deter_Indices(X_train_imputed, y_train_full, horizon=PREDICTION_HORIZON, var_threshold=optimal_var_q)\n","\n","        if 'Deter_States' in sets_to_run:\n","            predictor_data_iter['Deter_States'] = states_global_q\n","        if 'Deter_States_SecQ' in sets_to_run:\n","            predictor_data_iter['Deter_States_SecQ'] = states_sector_q\n","        if 'Deter_States_VarQ' in sets_to_run:\n","            predictor_data_iter['Deter_States_VarQ'] = states_variable_q\n","\n","        if 'Deter_PCA' in sets_to_run:\n","            predictor_data_iter['Deter_PCA'], loadings = generate_PCA_Factors_Binary(states_global_q, n_factors=8)\n","            oos_loadings[PREDICTION_HORIZON]['Deter_PCA'][forecast_date] = loadings\n","        if 'Deter_PCA_SecQ' in sets_to_run:\n","            predictor_data_iter['Deter_PCA_SecQ'], loadings = generate_PCA_Factors_Binary(states_sector_q, n_factors=8)\n","            oos_loadings[PREDICTION_HORIZON]['Deter_PCA_SecQ'][forecast_date] = loadings\n","        if 'Deter_PCA_VarQ' in sets_to_run:\n","            predictor_data_iter['Deter_PCA_VarQ'], loadings = generate_PCA_Factors_Binary(states_variable_q, n_factors=8)\n","            oos_loadings[PREDICTION_HORIZON]['Deter_PCA_VarQ'][forecast_date] = loadings\n","\n","        if 'Deter_Avg' in sets_to_run:\n","            predictor_data_iter['Deter_Avg'] = states_global_q.mean(axis=1).to_frame(name='Deter_Avg')\n","        if 'Deter_Avg_SecQ' in sets_to_run:\n","            predictor_data_iter['Deter_Avg_SecQ'] = states_sector_q.mean(axis=1).to_frame(name='Deter_Avg_SecQ')\n","        if 'Deter_Avg_VarQ' in sets_to_run:\n","            predictor_data_iter['Deter_Avg_VarQ'] = states_variable_q.mean(axis=1).to_frame(name='Deter_Avg_VarQ')\n","\n","        if 'Full_BestPred' in sets_to_run:\n","            predictor_data_iter['Full_BestPred'] = X_train_scaled\n","        if 'Deter_States_BestPred' in sets_to_run:\n","            predictor_data_iter['Deter_States_BestPred'] = states_global_q\n","\n","\n","        if i == 0:\n","            print(f\"\\n*** OPTIMIZING C VALUES ON FIRST TRAINING SAMPLE ***\")\n","            print(f\"Date: {forecast_date.date()}, Training data up to: {train_end_date.date()}\")\n","            print(f\"Using {len(C_values)} C values from {C_values[0]:.6f} to {C_values[-1]:.6f}\")\n","            print(f\"Will optimize C separately for each of the {len(sets_to_run)} predictor sets: {sets_to_run}\")\n","\n","            for pred_set_name in sets_to_run:\n","                print(f\"\\n  --- Processing {pred_set_name} ---\")\n","                X_train_raw = predictor_data_iter.get(pred_set_name)\n","\n","                if X_train_raw is None or X_train_raw.empty:\n","                    print(f\"    No data available, using default C=1.0\")\n","                    optimal_C_values_l1[pred_set_name] = 1.0\n","                    optimal_C_values_l2[pred_set_name] = 1.0\n","                    continue\n","\n","                # Add lags if specified\n","                X_train_lagged = add_lags(X_train_raw, LAGS_TO_ADD)\n","                X_train_lagged_final = X_train_lagged.dropna()\n","                X_train_final = X_train_lagged_final.loc[:train_end_date]\n","\n","                # Align with target\n","                common_index = y_train_full.index.intersection(X_train_final.index)\n","                y_train_aligned = y_train_full.loc[common_index]\n","                X_train_aligned = X_train_final.loc[common_index]\n","\n","                print(f\"    Data shape: {X_train_aligned.shape}, Target samples: {len(y_train_aligned)}\")\n","\n","                if len(X_train_aligned) > 50:  # Sufficient data for optimization\n","                    print(f\"    Optimizing C value using time series CV...\")\n","                    # Note: C optimization still uses the full training slice available at iter 0\n","                    optimal_C_l1 = optimize_C_value_first_sample_l1(X_train_aligned, y_train_aligned, C_values)\n","                    optimal_C_values_l1[pred_set_name] = optimal_C_l1\n","\n","                    optimal_C_l2 = optimize_C_value_first_sample_l2(X_train_aligned, y_train_aligned, C_values)\n","                    optimal_C_values_l2[pred_set_name] = optimal_C_l2\n","                    print(f\"    Optimal C (L1) for {pred_set_name}: {optimal_C_l1:.6f}\")\n","                    print(f\"    Optimal C (L2) for {pred_set_name}: {optimal_C_l2:.6f}\")\n","                else:\n","                    print(f\"    Insufficient data ({len(X_train_aligned)} samples), using default C=1.0\")\n","                    optimal_C_values_l1[pred_set_name] = 1.0\n","                    optimal_C_values_l2[pred_set_name] = 1.0\n","\n","            print(f\"\\n*** C OPTIMIZATION COMPLETE ***\")\n","            print(\"Summary of optimal C (L1) values by predictor set:\")\n","            for pred_set, opt_c in optimal_C_values_l1.items():\n","                print(f\"  {pred_set:20s}: C = {opt_c:.6f}\")\n","            print(\"Summary of optimal C (L2) values by predictor set:\")\n","            for pred_set, opt_c in optimal_C_values_l2.items():\n","                print(f\"  {pred_set:20s}: C = {opt_c:.6f}\")\n","\n","\n","        # Loop over the INTENDED sets\n","        for pred_set_name in sets_to_run:\n","            X_train_raw = predictor_data_iter.get(pred_set_name)\n","\n","            X_train_saved = X_train_raw.loc[:train_end_date]\n","            oos_training_datasets[pred_set_name].append(X_train_saved)\n","\n","            if X_train_raw is None or X_train_raw.empty:\n","                for model_name in MODELS_TO_RUN:\n","                    oos_probs[pred_set_name][model_name].append(np.nan)\n","                    oos_errors[pred_set_name][model_name].append(np.nan)\n","                continue\n","            else:\n","                X_train_lagged = add_lags(X_train_raw, LAGS_TO_ADD)\n","\n","            X_train_lagged_final = X_train_lagged.dropna()\n","            X_train_final = X_train_lagged_final.loc[:train_end_date]\n","\n","            X_predict_point = X_train_lagged_final.loc[[forecast_date]]\n","\n","\n","            for model_name, model_template in MODELS_TO_RUN.items():\n","                prob, error = np.nan, np.nan\n","                try:\n","                    common_index = y_train_full.index.intersection(X_train_final.index)\n","                    y_train_aligned = y_train_full.loc[common_index]\n","                    X_train_aligned = X_train_final.loc[common_index]\n","\n","                    if len(X_train_aligned) > max(LAGS_TO_ADD, default=0) + 20:\n","                        model_instance = clone(model_template)\n","                        params_to_set = {}\n","\n","                        if 'XGBoost' in model_name:\n","                            neg, pos = (y_train_aligned == 0).sum(), (y_train_aligned == 1).sum()\n","                            if pos > 0: params_to_set['scale_pos_weight'] = (neg / pos)\n","                        elif 'HGBoost' in model_name or 'RandomForest' in model_name or 'Logit' in model_name:\n","                            params_to_set['class_weight'] = 'balanced'\n","\n","                        if 'Logit_L1' in model_name and pred_set_name in optimal_C_values_l1:\n","                            params_to_set['C'] = optimal_C_values_l1[pred_set_name]\n","\n","\n","                        if 'Logit_L2' in model_name and pred_set_name in optimal_C_values_l2:\n","                            params_to_set['C'] = optimal_C_values_l2[pred_set_name]\n","\n","                        if params_to_set:\n","                            model_instance = model_instance.set_params(**params_to_set)\n","\n","                        X_predict_imputed = X_predict_point.reindex(columns=X_train_aligned.columns).ffill().bfill()\n","\n","                        if not X_predict_imputed.isna().any().any():\n","                            model_instance.fit(X_train_aligned, y_train_aligned)\n","\n","                            try:\n","                                if 'XGBoost' in model_name:\n","                                    booster = model_instance.get_booster()\n","                                    importance_dict = booster.get_score(importance_type='gain')\n","                                    importances = pd.Series(importance_dict, index=X_train_aligned.columns).fillna(0)\n","                                elif 'Logit' in model_name:\n","                                    importances = pd.Series(np.abs(model_instance.coef_[0]), index=X_train_aligned.columns)\n","                                else: # RandomForest, HGBoost\n","                                    importances = pd.Series(model_instance.feature_importances_, index=X_train_aligned.columns)\n","\n","\n","                            except Exception as e_imp:\n","                                print(f\"Could not get importances for {model_name} on {forecast_date.date()}: {e_imp}\")\n","                                pass\n","\n","                            if 'SAVE_MODELS' and forecast_date == forecast_dates[-1]:\n","                                print(\"Last iteration. Saving model...\")\n","                                if not LAGS_TO_ADD:\n","                                    model_filename = f'{pred_set_name}_{model_name}_model.pkl'\n","                                    traindata_filename = f'{pred_set_name}_{model_name}_traindata.pkl'\n","                                else:\n","                                    model_filename = f'{pred_set_name}_{model_name}_lagged_model.pkl'\n","                                    traindata_filename = f'{pred_set_name}_{model_name}_lagged_traindata.pkl'\n","\n","                                model_path = os.path.join(horizon_model_path, model_filename)\n","                                joblib.dump(model_instance, model_path)\n","\n","                                traindata_path = os.path.join(horizon_model_path, traindata_filename)\n","                                X_train_aligned.to_pickle(traindata_path)\n","\n","                                if pred_set_name in ['PCA_Factors_8', 'Deter_PCA']:\n","                                    loadings_for_model = oos_loadings[PREDICTION_HORIZON][pred_set_name][forecast_date]\n","                                    loadings_filename = f'{pred_set_name}_{model_name}_loadings.pkl'\n","                                    loadings_path = os.path.join(horizon_model_path, loadings_filename)\n","                                    loadings_for_model.to_pickle(loadings_path)\n","\n","                            prob = model_instance.predict_proba(X_predict_imputed)[:, 1][0]\n","                            error = (y_actual - prob)**2\n","\n","\n","                except Exception as e:\n","                    print(f\"Error in model {model_name} for set {pred_set_name}: {e}\")\n","                    pass\n","\n","                oos_probs[pred_set_name][model_name].append(prob)\n","                oos_errors[pred_set_name][model_name].append(error)\n","                oos_importances[pred_set_name][model_name].append(importances)\n","\n","        iter_end_time = time.time()\n","        print(f\" ({(iter_end_time - iter_start_time):.2f}s)\")\n","\n","    # Save results after each horizon's loop is complete\n","    print(f\"\\n--- Loop for h={PREDICTION_HORIZON} Finished ---\")\n","    results_to_save = {\n","        'probabilities': oos_probs,\n","        'squared_errors': oos_errors,\n","        'actuals': oos_actuals,\n","        'importances': oos_importances,\n","        'loadings': oos_loadings,\n","        'training_datasets': oos_training_datasets\n","    }\n","    joblib.dump(results_to_save, file_path)\n","    print(f\"Updated results saved to: {file_path}\")\n","\n","\n","# %% Report results\n","end_time = time.time()\n","print(f\"Total computation time: {end_time - start_time:.2f} seconds\")\n","print(\"\")\n","\n","print(\"\\n--- All Horizons Complete ---\")"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"18T8ENtyOeY6oZ-3Jk9e__GduSEjCUfk2","authorship_tag":"ABX9TyPNlBKBIVvkVF0WVYv8hx+e"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}